<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A simple adaptive optimizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <link rel="stylesheet" href="styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <h1>Part II: a simple adaptive optimizer</h1>

    <nav class="top-nav">
        <a href="index.html">Intro</a>
        <span class="separator">|</span>
        <a href="part1.html">Part I</a>
        <span class="separator">|</span>
        <a href="part2.html" class="active">Part II</a>
        <span class="separator">|</span>
        <a href="part3.html">Part III</a>
    </nav>

    <p class="body-text">
        We now examine a simple adaptive optimizer, which we call <em>Scalar RMSProp</em>:
        \[
            \begin{align}
            \nu_{t+1} = \beta_2 \, \nu_t + (1-\beta_2) \|\nabla L(w_t)\|^2, \quad \quad w_{t+1} = w_t - \frac{\eta}{\sqrt{\nu_{t+1}}} \nabla L(w_t)
            \end{align}
        \]
    </p>
    <p class="body-text">
        The algorithm maintains an exponential moving average, \(\nu_t\), of the squared gradient, and then takes gradient steps with the effective step size \(\eta / \sqrt{\nu_{t+1}}\).
        The algorithm is a simplification of RMSProp, and its analysis is a stepping stone to that of RMSProp.
    </p>
    <p class="body-text">
        We will answer the basic question: what is the optimizer adapting to?
    </p>
    <p class="body-text">
        First, to give some intuition, look at the loss, gradient norm, nu, and ESS:
    </p>
    <p class="body-text">
        See that the gradient norm fluctuates rapidly, causing nu and the ESS to also fluctuate.  Why is the gradient norm behaving like this?  (Remember, this is deterministic training.)
        The answer is that the gradient norm is fluctuating because the optimizer operating in an oscillatory edge of stability regime.
    </p>
    <p class="body-text">
        Let us understand these dynamics more precisely.  Recall that gradient descent oscillates "up the valley walls" if the sharpness \(S(w)\) is greater than the threshold \(2/\eta\).
        Since a step of Scalar RMSProp can be viewed as a step of gradient descent with learning rate \(\eta / \sqrt{\nu_{t+1}}\), we can see that Scalar RMSProp oscillates "up the valley walls" 
        if \(S(w)\) > \(2 \sqrt{\nu_{t+1}} / \eta\).
        Equivalently, if we define the <em>effective sharpness</em> as 
         \[
            S_{\text{eff}} := \eta S(w) / \sqrt{\nu_{t+1}}
         \]
        then Scalar RMSProp oscillates "up the valley walls" if \(S_{\text{eff}} > 2\).
    </p>
    <p class="body-text">
       Similarly, such oscillations in turn act to decrease the effective sharpness, a form of negative feedback which stabilizes optimization.
       There are two mechanisms by which this negative feedback occurs.
       First, the oscillations act to decrease the sharpness, as we have seen already for gradient descent.
       Second, the oscillations act to decrease the step size, as we will see.
    </p>
    <p class="body-text">
        To see how the step size decreases, recall that the step size of Scalar RMSProp is \(\eta / \sqrt{\nu_{t+1}}\).
        Since \(\nu_{t+1}\) is an exponentially moving average of the squared gradient, it will increase over time if the gradient is large.
    </p>
    <p class="body-text">
        The net effect of these two mechanisms is that the the effective sharpness \(S_{\text{eff}}\) equilibrates around the value 2, as you can see in the following plot:
    </p>
    <p class="body-text">
        Indeed, this is the equilibrium condition that the entire dynamics of the system revolve around.
    </p>
    <p class="body-text">
        The fine-grained dynamics of the system are complex and challenging to analyze, even in the relatively simple setting where just one eigenvalue is at the edge of stability.
        Indeed, below you can see a zoom in of one period of the dynamics.
    </p>
    <p class="body-text">
        Fortunately, as with gradient descent, we will see that the <em>time-averaged</em> dynamics are much easier to analyze.
        Let us consider the case where just one eigenvalue is at the edge of stability.
        <div class="figure" style="width:25%; float: right; margin: 0 0 16px 32px;">
            <img src="media/divergence-illustration.png">
            <div class="caption">The iterate \(w\) is displaced from \(\overline{w}\) along the direction \(u\) with magnitude \(x\).</div>
        </div>
        Before, we computed that the time-averaged gradient is given by
        \[
            \underset{\color{red} \begin{array}{c} \text{time-averaged} \\ \text{gradient} \end{array}}{\mathbb{E}[\nabla L(w_t)]} 
                = \underset{\color{red} \begin{array}{c} \text{gradient at time-} \\ \text{averaged iterate} \end{array}}{\nabla L(\overline{w}_t)} + \underset{\color{red} \begin{array}{c} \text{implicit sharpness} \\ \text{reduction} \end{array}}{\tfrac{1}{2} \mathbb{E}[x_t^2] \nabla S(\overline{w}_t)}
        \]
    </p>
    <p class="body-text">
        Similarly, by using the first two terms in the Taylor expansion, we can compute the time-averaged gradient norm as:
        \[
        \underset{\color{red} \begin{array}{c} \text{time-averaged} \\[-0.2em] \text{gradient norm}^2 \end{array}}{\mathbb{E}[\|\nabla L(w_t)\|^2]} = \underset{\color{red} \begin{array}{c} \text{gradient norm}^2 \text{ at time-} \\ \text{averaged iterate} \end{array}}{\|\nabla L(\overline{w}_t)\|^2} 
        + \underset{\color{red} \begin{array}{c} \text{contribution from} \\ \text{oscillations} \end{array}}{\mathbb{E}[x_t^2] S(\overline{w}_t)^2}
        \]
    </p>
    <p class="body-text">
        Based on these time-averages, we make the ansatz that the time-averaged dynamics of \(w, \nu \) can be described by a central flow \(w(t), \nu(t) \) of the following functional form:
        \[
            \frac{dw}{dt} = -\frac{\eta}{\sqrt{\nu}} \color{red}{\underbrace{\color{black}{\left[ \nabla L(w) + \frac{1}{2} \sigma^2(t) \nabla S(w) \right]}}_{\text{time-averaged gradient}}} \color{black},
             \quad \quad 
              \frac{d \nu}{dt} =  \frac{1-\beta_2}{\beta_2} \left[ \color{red}{\underbrace{\color{black}{ \| \nabla L(w) \|^2 + \sigma^2 (t) S(w)^2}}_{\text{time-averaged gradient norm}^2}} \color{black} - \nu \right],
        \]
    </p>
    <p class="body-text">
        where \(\sigma(t)\) models the instantaneous variance of the oscillations at time \(t\).  See the paper for why we do \(\frac{1 - \beta_2}{\beta_2}\) rather than \(1- \beta_2\).
    </p>
    <p class="body-text">
        As with gradient descent, there is a unique value of \(\sigma^2(t)\) that causes the time derivative of the effective sharpness to be zero.
        We set \(\sigma^2(t)\) to this unique value, which defines the central flow in the case of a single unstable eigenvalue.
        A similar derivation can be done for the case of multiple unstable eigenvalues, which we leave to the paper.
    </p>
    <p class="body-text">
        As you can see here, this central flow matches the trajectory of Scalar RMSProp quite well.
        Though the exact dynamics are hard to understand, we see that the time-averaged dynamics are much more tractable.
    </p>
    <p class="body-text">
        Let us now use the central flow formalism to understand the precise sense in which Scalar RMSProp is "adaptive."
    </p>
    <p class="body-text">
        When Scalar RMSProp is at the edge of stability, the effective sharpness \(S_{\text{eff}} := \eta S(w) / \sqrt{\nu_{t+1}}\) is fixed at 2.
        We can rearrange this condition to get a condition on the effective step size:
        \[
            \underset{\color{red} \begin{array}{c} \text{effective} \\[-0.2em] \text{step size} \end{array}}{\frac{\eta}{\sqrt{\nu}}} = \frac{2}{S(w)}.
        \]
    </p>
    <p class="body-text">
        That is, at the edge of stability, the effective step size is fixed at \(\frac{2}{S(w)}\).
        Notably, the value \(\frac{2}{S(w)}\) is the <em>largest stable step size</em> for gradient descent at the location \(w\) in weight space.
        Thus, at EOS, the dynamics of Scalar RMSProp automatically set the step size to the largest stable step size.
    </p>
    <p class="body-text">
        Of course, it would be possible for an optimizer to <em>manually</em> compute the sharpness \(S(w)\) at each step, and manually set the step size to \(\frac{2}{S(w)}\).
        However, this would be computationally expensive.  What is interesting is that Scalar RMSProp does the same thing, but <em>efficiently</em>, requiring only one gradient query per iteration,
        which is the same cost as a step of gradient descent.
    </p>
    <p class="body-text">
        Further, note that even comprehending this step size strategy requires some notion of time averaging.
        The effective step size of discrete Scalar RMSProp is not <em>exactly</em> fixed at \(\frac{2}{S(w)}\), but rather fluctuates around this value.
        The important thing is that it is \(2/\eta\) <em>on average over time</em>.
        The central flow formalism allows us to reason about this behavior.
    </p>
</body>
</html> 