<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A simple adaptive optimizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <link rel="stylesheet" href="styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="js/mathjax-config.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="js/video-controls.js"></script>
</head>
<body>
    <nav class="top-nav">
        <a href="index.html">Introduction</a>
        <span class="separator">|</span>
        <a href="part1.html">Part I</a>
        <span class="separator">|</span>
        <a href="part2.html" class="active">Part II</a>
        <span class="separator">|</span>
        <a href="part3.html">Part III</a>
        <span class="separator">|</span>
        <a href="conclusion.html">Conclusion</a>
    </nav>

    <h1>Part II: a simple adaptive optimizer</h1>

    <p class="body-text">
        We now examine a simple adaptive optimizer, which we call <em>Scalar RMSProp</em>:
        \[
            \begin{align}
            \nu_{t} = \beta_2 \, \nu_t + (1-\beta_2) \|\nabla L(w_t)\|^2, \quad \quad w_{t+1} = w_t - \frac{\eta}{\sqrt{\nu_{t}}} \nabla L(w_t)
            \end{align}
        \]
    </p>
    <p class="body-text">
        The algorithm maintains an exponential moving average, \(\nu_t\), of the squared gradient, and then takes gradient steps with the effective step size \(\eta / \sqrt{\nu_{t+1}}\).
        The algorithm is a simplification of RMSProp, and its analysis is a stepping stone to that of RMSProp.
    </p>
    <p class="body-text">
        We will answer the basic question: what is the optimizer adapting to?
    </p>
    <p class="body-text">
        First, to give some intuition, look at the loss, gradient norm, nu, and ESS:
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%"">
                <source src="media/scalar-rmsprop-intuition.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">Intuition for the dynamics of Scalar RMSProp.</div>
    </div>
    <p class="body-text">
        See that the gradient norm fluctuates rapidly, causing nu and the ESS to also fluctuate.  Why is the gradient norm behaving like this?  (Remember, this is deterministic training.)
        The answer is that the gradient norm is fluctuating because the optimizer operating in an oscillatory edge of stability regime.
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%"">
                <source src="media/scalar-rmsprop-zoom.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">Zoom in.</div>
    </div>
    <p class="body-text">
        Let us understand these dynamics more precisely.  Recall that gradient descent oscillates "up the valley walls" if the sharpness \(S(w)\) is greater than the threshold \(2/\eta\).
        Since a step of Scalar RMSProp can be viewed as a step of gradient descent with learning rate \(\eta / \sqrt{\nu_{t+1}}\), we can see that Scalar RMSProp oscillates "up the valley walls" 
        if \(S(w)\) > \(2 \sqrt{\nu_{t+1}} / \eta\).
        Equivalently, if we define the <em>effective sharpness</em> as 
         \[
            S_{\text{eff}} := \eta S(w) / \sqrt{\nu_{t+1}}
         \]
        then Scalar RMSProp oscillates "up the valley walls" if \(S_{\text{eff}} > 2\).
    </p>
    <p class="body-text">
       Similarly, such oscillations in turn act to decrease the effective sharpness, a form of negative feedback which stabilizes optimization.
       There are two mechanisms by which this negative feedback occurs.
       First, the oscillations act to decrease the sharpness, as we have seen already for gradient descent.
       Second, the oscillations act to decrease the step size, as we will see.
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%"">
                <source src="media/scalar-rmsprop-eos.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">Zoom in.</div>
    </div>
    <p class="body-text">
        To see how the step size decreases, recall that the step size of Scalar RMSProp is \(\eta / \sqrt{\nu_{t+1}}\).
        Since \(\nu_{t+1}\) is an exponentially moving average of the squared gradient, it will increase over time if the gradient is large.
    </p>
    <p class="body-text">
        The net effect of these two mechanisms is that the the effective sharpness \(S_{\text{eff}}\) equilibrates around the value 2, as you can see in the following plot:
    </p>
    <p class="body-text">
        Indeed, this is the equilibrium condition that the entire dynamics of the system revolve around.
    </p>
    <p class="body-text">
        The fine-grained dynamics of the system are complex and challenging to analyze, even in the relatively simple setting where just one eigenvalue is at the edge of stability.
        Indeed, below you can see a zoom in of one period of the dynamics.
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%"">
                <source src="media/scalar-rmsprop-eos-zoom.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">Zoom in.</div>
    </div>
    <p class="body-text">
        Fortunately, as with gradient descent, we will see that the <em>time-averaged</em> dynamics are much easier to analyze.
        Let us consider the case where just one eigenvalue is at the edge of stability.
        <div class="figure" style="width:25%; float: right; margin: 0 0 16px 32px;">
            <img src="media/divergence-illustration.png">
            <div class="caption">The iterate \(w\) is displaced from \(\overline{w}\) along the direction \(u\) with magnitude \(x\).</div>
        </div>
        Before, we computed that the time-averaged gradient is given by
        \[
            \underset{\color{red} \begin{array}{c} \text{time-averaged} \\ \text{gradient} \end{array}}{\mathbb{E}[\nabla L(w_t)]} 
                = \underset{\color{red} \begin{array}{c} \text{gradient at time-} \\ \text{averaged iterate} \end{array}}{\nabla L(\overline{w}_t)} + \underset{\color{red} \begin{array}{c} \text{implicit sharpness} \\ \text{reduction} \end{array}}{\tfrac{1}{2} \mathbb{E}[x_t^2] \nabla S(\overline{w}_t)}
        \]
    </p>
    <p class="body-text">
        Similarly, by using the first two terms in the Taylor expansion, we can compute the time-averaged gradient norm as:
        \[
        \underset{\color{red} \begin{array}{c} \text{time-averaged} \\[-0.2em] \text{gradient norm}^2 \end{array}}{\mathbb{E}[\|\nabla L(w_t)\|^2]} = \underset{\color{red} \begin{array}{c} \text{gradient norm}^2 \text{ at time-} \\ \text{averaged iterate} \end{array}}{\|\nabla L(\overline{w}_t)\|^2} 
        + \underset{\color{red} \begin{array}{c} \text{contribution from} \\ \text{oscillations} \end{array}}{\mathbb{E}[x_t^2] S(\overline{w}_t)^2}
        \]
    </p>
    <p class="body-text">
        Based on these time-averages, we make the ansatz that the time-averaged dynamics of \(w, \nu \) can be described by a central flow \(w(t), \nu(t) \) of the following functional form:
        \[
            \frac{dw}{dt} = -\frac{\eta}{\sqrt{\nu}} \color{red}{\underbrace{\color{black}{\left[ \nabla L(w) + \frac{1}{2} \sigma^2(t) \nabla S(w) \right]}}_{\text{time-averaged gradient}}} \color{black},
             \quad \quad 
              \frac{d \nu}{dt} =  \frac{1-\beta_2}{\beta_2} \left[ \color{red}{\underbrace{\color{black}{ \| \nabla L(w) \|^2 + \sigma^2 (t) S(w)^2}}_{\text{time-averaged gradient norm}^2}} \color{black} - \nu \right],
        \]
    </p>
    <p class="body-text">
        where \(\sigma(t)\) models the instantaneous variance of the oscillations at time \(t\).  See the paper for why we do \(\frac{1 - \beta_2}{\beta_2}\) rather than \(1- \beta_2\).
    </p>
    <p class="body-text">
        As with gradient descent, there is a unique value of \(\sigma^2(t)\) that causes the time derivative of the effective sharpness to be zero.
        We set \(\sigma^2(t)\) to this unique value, which defines the central flow in the case of a single unstable eigenvalue.
        A similar derivation can be done for the case of multiple unstable eigenvalues, which we leave to the paper.
    </p>
    <p class="body-text">
        As you can see here, this central flow matches the trajectory of Scalar RMSProp quite well.
        Though the exact dynamics are hard to understand, we see that the time-averaged dynamics are much more tractable.
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%"">
                <source src="media/scalar-rmsprop-central.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">Zoom in.</div>
    </div>
    <p class="body-text">
        We can also predict \(\Sigma(t)\).
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%"">
                <source src="media/scalar-rmsprop-predict-sigma.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">Zoom in.</div>
    </div>
    <p class="body-text">
        Let us now use the central flow formalism to understand the precise sense in which Scalar RMSProp is "adaptive."
    </p>
    <p class="body-text">
        When Scalar RMSProp is at the edge of stability, the effective sharpness \(S_{\text{eff}} := \eta S(w) / \sqrt{\nu_{t+1}}\) is fixed at 2.
        We can rearrange this condition to get a condition on the effective step size:
        \[
            \underset{\color{red} \begin{array}{c} \text{effective} \\[-0.2em] \text{step size} \end{array}}{\frac{\eta}{\sqrt{\nu}}} = \frac{2}{S(w)}.
        \]
    </p>
    <p class="body-text">
        That is, at the edge of stability, the effective step size is fixed at \(\frac{2}{S(w)}\).
        Notably, the value \(\frac{2}{S(w)}\) is the <em>largest stable step size</em> for gradient descent at the location \(w\) in weight space.
        Thus, at EOS, the dynamics of Scalar RMSProp automatically set the step size to the largest stable step size.
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%" style="width: 80%; display: block; margin: 0 auto;">
                <source src="media/scalar-rmsprop-ess.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">Zoom in.</div>
    </div>
    <p class="body-text">
        Of course, it would be possible for an optimizer to <em>manually</em> compute the sharpness \(S(w)\) at each step, and manually set the step size to \(\frac{2}{S(w)}\).
        However, this would be computationally expensive.  What is interesting is that Scalar RMSProp does the same thing, but <em>efficiently</em>, requiring only one gradient query per iteration,
        which is the same cost as a step of gradient descent.
    </p>
    <p class="body-text">
        Further, note that even comprehending this step size strategy requires some notion of time averaging.
        The effective step size of discrete Scalar RMSProp is not <em>exactly</em> fixed at \(\frac{2}{S(w)}\), but rather fluctuates around this value.
        The important thing is that it is \(2/\eta\) <em>on average over time</em>.
        The central flow formalism allows us to reason about this behavior.
    </p>
    <p class="body-text">
        So, is that it?  Should we think of Scalar RMSProp as just a clever way to compute the step size?  
        No, because there is also the implicit sharpness reduction term which arises from the oscillations.
        We can formalize this using the central flow.  In general, the central flow over the two variables \(w, \nu\).
        However, at EOS, because \(\eta / \sqrt{\nu} = 2 / S(w)\), we can eliminate \(\nu\) from the central flow, and write it as a flow over \(w\) alone:
    </p>
    <p class="body-text">
        \[
            \frac{dw}{dt} = -\underset{\color{red} \text{adapt step size}}{\frac{2}{S(w)}} 
                {\left[ \nabla L(w) +  \underset{\color{red} \text{implicitly reduce sharpness}}{\frac{1}{2} \sigma^2(t) \nabla S(w)} \right]}
        \]
    </p>
    <p class="body-text">
        In other words, while at EOS, the trajectory of Scalar RMSProp is essentially equivalent to that of the following simpler-to-understand algorithm:
        <div class="boxed-text">
            At each iteration, compute the sharpness \(S(w)\), and take a gradient step with step size \(\frac{2}{S(w)}\) on a sharpness-regularized objective.
        </div>
    </p>
    <p class="body-text">
        You can see the paper for the precise form of the implicit sharpness regularizer.
    </p>
    <p class="body-text">
        This implicit sharpness regularization is crucial to the optimization efficacy of Scalar RMSProp.
    </p>
    <p class="body-text">
        Over the short term:
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%" style="width: 100%; display: block; margin: 0 auto;">
                <source src="media/scalar-rmsprop-acc-via-reg-flow-initial.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">Zoom in.</div>
    </div>
    <p class="body-text">
        Over the long term:
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%" style="width: 100%; display: block; margin: 0 auto;">
                <source src="media/scalar-rmsprop-acc-via-reg-flow.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">Zoom in.</div>
    </div>
    <p class="body-text">
        Learning rates too:
        <div class="figure">
            <div class="video-container">
                <video width="100%" style="width: 100%; display: block; margin: 0 auto;">
                    <source src="media/scalar-rmsprop-acc-via-reg-lr.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <div class="video-overlay">Click to play</div>
            </div>
            <div class="caption">Zoom in.</div>
        </div>
    </p>
</html> 