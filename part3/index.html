<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How does RMSProp work?</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <link rel="stylesheet" href="../styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="../js/mathjax-config.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../js/video-controls.js"></script>
</head>
<body>
    <nav class="top-nav">
        <div class="top-nav-left">
            <a href="../central-flows-draft.pdf">ðŸ“„ Paper</a>
            <!-- <a href="https://arxiv.org/abs/2410.24206">ðŸ“„ Paper</a> -->
        </div>
        <div class="top-nav-center">
            <a href="../">Introduction</a>
            <span class="separator">|</span>
            <a href="../part1">Part I</a>
            <span class="separator">|</span>
            <a href="../part2">Part II</a>
            <span class="separator">|</span>
            <a href="../part3" class="active">Part III</a>
            <!-- <span class="separator">|</span>
            <a href="../conclusion">Conclusion</a> -->
        </div>
        <div class="top-nav-right">
            <a href="https://github.com/locuslab/central_flows">ðŸ’» Code</a>
        </div>
    </nav>

    <p class="companion-text">
        This is the companion website for the paper <a href="../central-flows-draft.pdf">Understanding Optimization in Deep Learning with Central Flows</a>, published at ICLR 2025.
        <!-- This is the companion website for the paper <a href="https://arxiv.org/abs/2410.24206">Understanding Optimization in Deep Learning with Central Flows</a>, published at ICLR 2025. -->
     </p>

    <h1>Part III: how does RMSProp work?</h1>

    <p class="body-text">
        We now study <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">RMSProp</a>, which is equivalent to Adam without momentum:
        \[
            \underset{\color{red}{\text{update EMA of squared gradient}\,\rule[18pt]{0pt}{0pt}}}
                     {\nu_{t} = \beta_{2} \, \nu_{t-1} + (1-\beta_{2}) \nabla L(w_t)^{\odot 2}}, \quad \quad
            \underset{\color{red}{\text{take preconditioned gradient step}}}{w_{t+1} = w_t - \frac{\eta}{\sqrt{\nu_{t}} }\odot \nabla L(w_t)}.
        \]
    </p>
    <p class="body-text">
        RMSProp maintains an exponential moving average (EMA) \(\nu\) of the element-wise squared gradient \(\nabla L(w)^{\odot 2}\), and then takes a step using per-coordinate step sizes of \(\eta / \sqrt{\nu}\).
        There is an equivalent viewpoint that is sometimes more convenient to work with: if we define preconditioned gradient descent as: \( w_{t+1} = w_t - P^{-1} \nabla L(w_t)\),
        then RMSProp is preconditioned gradient descent with the dynamic preconditioner \(P_t = \text{diag}(\sqrt{\nu_t} / \eta)\).
    </p>
    <p class="body-text">
        <a href="https://arxiv.org/abs/1412.6980">Adam</a> uses the same preconditioning strategy, and has achieved massive success in deep learning.  However, it is a <a href="https://parameterfree.com/2020/12/06/neural-network-maybe-evolved-to-make-adam-the-best-optimize">mystery</a> why this particular preconditioner should work so well.
        Common folklore is that Adam/RMSProp adapts to the "curvature", i.e. Hessian, of the objective function.
        However, it is unclear why this should be the case, since Adam/RMSProp updates its preconditioner using the <em>gradient</em> \( \nabla L \), not the Hessian \( \nabla^2 L \).
    </p>
    <p class="body-text">
        In this section, we will use the central flows framework to understand RMSProp.
        We will see that RMSProp <em>does</em> adapt to the local Hessian after all &mdash; but the reason is inextricably tied to the optimizer's oscillatory dynamics, which have not been previously studied.
    </p>
    <h2>The dynamics of RMSProp</h2>
    <p class="body-text">
        Let's start by getting some intuition for RMSProp's behavior.
        We'll train a ResNet on a subset of CIFAR-10 using deterministic RMSProp.
        As you can see, the loss curve looks pretty well-behaved:
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%" style="width: 40%; display: block; margin: 0 auto;">
                <source src="../media/rmsprop-loss.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">We train a ResNet on a subset of CIFAR-10 using deterministic RMSProp with \(\eta = \) 2e-5 and \(\beta_2 = 0.99\).</div>
    </div>
    <p class="body-text">
        Yet, even though it looks like smooth sailing, there are interesting things happening beneath the surface.  Recall that RMSProp maintains an EMA \( \nu_t\) of the element-wise squared gradient  \( \nabla L(w_t)^{\odot2} \).
        Let's examine the dynamics of these two quantities at several individual coordinates:
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%"">
                <source src="../media/rmsprop-discrete-nu.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">The individual entries of the squared gradient (faint lines) fluctuate rapidly, causing their EMA (dark lines) to also fluctuate.</div>
    </div>
    <p class="body-text">
        You can see that the entries of the squared gradient \( \nabla L(w_t)^{\odot 2} \) are fluctuating rapidly.  This causes their EMA \( \nu_t \) to also fluctuate (although less so).
        Since this EMA is used to determine the algorithm's step sizes \( \eta / \sqrt{\nu_t} \), we need to understand the origin of this behavior if we want to understand how RMSProp sets its step sizes / preconditioner.
    </p>
    <p class="body-text">
        These fluctuations in the gradient arise because, like gradient descent, RMSProp is operating in an oscillatory <em>edge of stability</em> regime.
        To understand why RMSProp would oscillate, consider running preconditioned gradient descent: \[ w_{t+1} = w_t - P^{-1} \nabla L(w_t) \] on a quadratic function with Hessian \( H \).
        The resulting dynamics are controlled by the <em>effective Hessian </em> \( P^{-1} H \).
        If this matrix has any eigenvalue greater than 2, then preconditioned gradient descent will oscillate along the corresponding (right) eigenvector, as illustrated in the animation below:
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%" style="width: 40%; display: block; margin: 0 auto;">
                <source src="../media/preconditioned-gd-quadratic.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">We run preconditioned gradient descent on a quadratic function \(\frac{1}{2} w^T H w\), using a preconditioner \(P\) for which \(\lambda_\max(P^{-1} H) > 2\). 
            The eigenvectors of the effective Hessian \(P^{-1} H \) are shown as \( \mathbf{q}_1 \) and \( \mathbf{q}_2 \).
            Observe that the optimizer diverges along the direction \( \mathbf{q}_1 \), not the top Hessian eigenvector (which can be visualized as the top principal axis of the quadratic).
        </div>
    </div>
    <p class="body-text">
        While deep learning objectives are not globally quadratic, a <em>local</em> quadratic Taylor approximation suggests that RMSProp will oscillate
         if any eigenvalues of the <em>current</em> effective Hessian \( P_t^{-1} H(w_t) = \text{diag}(\eta / \sqrt{\nu_t}) \, H(w_t) \) exceed the critical threshold 2.
    </p>
    <p class="body-text">
        Let's examine how the eigenvalues of the effective Hessian evolve, under the dynamics of RMSProp:
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%" style="width: 80%; display: block; margin: 0 auto;">
                <source src="../media/rmsprop-eos.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">As RMSProp trains, the top eigenvalue of the effective Hessian \( \text{diag}(\eta / \sqrt{\nu}) H(w_t)\) equilibrates around the value 2.</div>
    </div>
    <p class="body-text">
        Initially, all eigenvalues are below 2, and the optimizer does not oscillate.
        Once the top eigenvalue rises to the threshold 2, RMSProp enters an <em>edge of stability</em> regime where optimizer oscillates along the unstable eigenvector(s) of the effective Hessian, and such oscillations induce reduction of the corresponding eigenvalue(s).
        The net effect is that the top eigenvalues equilibrate around the value 2.
    </p>
    <p class="body-text">
        For RMSProp, there are two distinct mechanisms by which oscillations along the unstable eigenvectors reduce the unstable eigenvalues of the effective Hessian \( \text{diag}(\eta / \sqrt{\nu}) \, H(w_t)\).
        First, such oscillations induce implicit regularization of the Hessian \(H(w_t)\), as is revealed by considering a local cubic Taylor expansion of the objective (recall <a href="../part1">Part I</a>).
        Second, such oscillations increase the gradient (as will be made clear below) and hence \(\nu_t\).
        RMSProp stabilizes itself using a combination of both mechanisms.
    </p>
    <p class="body-text">
        While analyzing the exact dynamics is challenging, as in <a href="../part1">Part I</a> and <a href="../part2">Part II</a> we will see that analyzing the <em>time-averaged</em> dynamics is much easier.
    </p>
    <h2>Deriving the central flow</h2>
    <p class="body-text">
        So long as RMSProp is stable (i.e. all eigenvalues of the effective Hessian are less than 2), its trajectory is reasonably well-approximated by the following differential equation, which we call the <em>stable flow</em>:
        \[
            \begin{align}
                \frac{dw}{dt} = -\frac{\eta}{\sqrt{\nu}} \nabla L(w), \quad \quad \frac{d \nu}{dt} = \left(\frac{1 - \beta_2}{\beta_2} \right) \nabla L(w)^{\odot 2}. \label{eq:stable-flow}
            \end{align}
        \]
    </p>
    <div class="figure" style="width:30%; float: right; margin: 0 0 16px 32px;">
        <img src="../media/divergence-illustration-flow-delta.png">
        <div class="caption">We model the iterate \(w_t\) as displaced from the central flow \(w(t)\) by some oscillation \(\delta_t\)</div>
    </div>
    <p class="body-text">
        Upon reaching the edge of stability, RMSProp starts to oscillate, and deviates from this path.  
        We will now derive a <em>central flow</em> that characterizes the time-averaged trajectory of RMSProp even at EOS.
    </p>
    <p class="body-text">
        As in <a href="../part1">Part I</a>, we model RMSProp \(\{w_t\}\) as being displaced from the central flow \(w(t)\) by some oscillation \(\delta_t\) that lies
        within the span of the eigenvectors that are at the edge of stability:
        \[
            \underset{\color{red} \text{iterate}}{w_t} = \underset{\color{red} \begin{array}{c}  \text{time-averaged} \\[-4pt] \text{iterate} \end{array} }{w(t)} + \underset{\color{red} \begin{array}{c} \text{oscillation} \end{array} }{\delta_t} \quad \text{where} \quad
             \underset{\color{red} \begin{array}{c} \substack{\; \; \, \delta_t \text{ is eigenvector of effective} \\ \text{Hessian with eigenvalue 2} }\end{array} }{\text{diag}(\eta / \sqrt{\nu}) \, H(w(t)) \, \delta_t = 2 \, \delta_t}.
        \]
    </p>
    <p class="body-text">
        Recall from <a href="../part1">Part I</a> that the time-averaged gradient is then approximately:
        \[
        \underbrace{\mathbb{E}[\nabla L(w_t)]}_{\color{red}{\substack{\text{time-average} \\ \text{of gradient}}}} \approx
             \underbrace{\nabla L(w(t))}_{\color{red}{\substack{\text{gradient at time-} \\ \text{averaged iterate}}}}
             + \underbrace{\tfrac{1}{2} \nabla \langle \Sigma(t), H(w(t)) \rangle}_{\color{red}{\substack{\text{implicit curvature} \\ \text{reduction}}}},
            
        \]
        where \(\Sigma(t) = \mathbb{E}[\delta_t \delta_t^T] \) models the instantaneous covariance matrix of the oscillations at time \(t\).
    </p>
    <p class="body-text">
        We can similarly compute the time-average of the squared gradient, by using the first two terms in the Taylor expansion of \(\nabla L\),
        and by using that the oscillation \(\delta_t\) is an eigenvector of the effective Hessian with eigenvalue 2:
        \[
        \begin{align}
        \underbrace{\mathbb{E}[\nabla L(w_t)^{\odot 2}]}_{\color{red}{\substack{\text{time-average of} \\ \text{ squared gradient}}}}
        \approx \underbrace{\nabla L(w(t))^{\odot 2}}_{\color{red}{\substack{\text{squared gradient at} \\ \text{time-averaged iterate}}}} + \underbrace{ \frac{4 \nu}{\eta^2} \odot \text{diag}[\Sigma(t)] }_{\color{red}{\substack{\text{contribution from oscillations}}}}. \label{eq:time-average-sq-gradient}
        \end{align}
        \]
    </p>
    <!-- <p class="body-text">
        We can similarly compute the time-average of the squared gradient, by using the first two terms in the Taylor expansion of \(\nabla L\):
        \[
        \begin{align}
        \underbrace{\mathbb{E}[\nabla L(w_t)^{\odot 2}]}_{\color{red}{\substack{\text{time-average of} \\ \text{ squared gradient}}}}
        \approx \underbrace{\nabla L(w(t))^{\odot 2}}_{\color{red}{\substack{\text{squared gradient at} \\ \text{time-averaged iterate}}}} + \underbrace{ \mathbb{E}[(H(w(t)) \; \delta_t)^{\odot 2} ] }_{\color{red}{\substack{\text{contribution from oscillations}}}}.
        \end{align}
        \]
    </p>
    <p class="body-text">
        Because \(\delta_t\) is an eigenvector of the effective Hessian \( \text{diag}(\eta / \sqrt{\nu}) \, H(w(t)) \) with eigenvalue 2, the rightmost term simplifies to:
        \[
        \begin{align}
        \underbrace{\mathbb{E}[\nabla L(w_t)^{\odot 2}]}_{\color{red}{\substack{\text{time-average of} \\ \text{ squared gradient}}}}
        \approx \underbrace{\nabla L(w(t))^{\odot 2}}_{\color{red}{\substack{\text{squared gradient at} \\ \text{time-averaged iterate}}}} + \underbrace{ \frac{4 \nu}{\eta^2} \odot \text{diag}[\Sigma(t)] }_{\color{red}{\substack{\text{contribution from oscillations}}}}. \label{eq:time-average-sq-gradient}
        \end{align}
        \]
    </p> -->
    <p class="body-text">
        This calculation makes clear that larger oscillations (i.e. larger diagonal entries of \(\Sigma(t)\)) <em>increase</em> the time-averaged squared gradient.
    </p>

    <p class="body-text">
        Based on these time-averages, we make the ansatz that the time-averaged dynamics of RMSProp can be captured by a central flow \((w(t), \nu(t)) \) of the following functional form:
        \[
            \begin{align}
                \frac{dw}{dt} = -\frac{\eta}{\sqrt{\nu}} \odot \left[\underbrace{\nabla L(w) + \tfrac{1}{2} \nabla \langle \Sigma(t), H(w) \rangle}_{\color{red} \text{time-average of gradient} }\right], \quad \quad
                \frac{d\nu}{dt} = \frac{1-\beta_2}{\beta_2} \left[\underbrace{\nabla L(w)^{\odot 2} + \frac{4\nu}{\eta^2} \odot \text{diag}[\Sigma(t)]}_{\color{red} \text{time-average of squared gradient} } - \nu \right ].
                \label{eq:ansatz}
            \end{align}
        \]
        As in <a href="../part1">Part I</a>, to determine \(\Sigma(t)\), we impose three natural conditions, and show that only one value of \(\Sigma(t)\) satisfies all three conditions.
        This value is the solution to a certain semidefinite complementarity problem (SDCP).
        The RMSProp central flow is defined as equation \eqref{eq:ansatz} with this value of \(\Sigma(t)\).
        Note that this central flow automatically reduces to the stable flow \eqref{eq:stable-flow} when all eigenvalues of the effective Hessian are less than 2.
    </p>
    
    <p class="body-text">
        Let's take a look at the central flow in action.
        You can see that under the central flow, each eigenvalue of the effective Hessian rises to 2 and then stays locked at this value:
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%" style="width: 100%; display: block; margin: 0 auto;">
                <source src="../media/rmsprop-centralflow.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">Under the central flow, each eigenvalue of the effective Hessian rises to 2 and then stays locked at this value.
        </div>
    </div>
    <p class="body-text">

    </p>
    <p class="body-text">
        We can furthermore assess whether the central flow's \(\Sigma(t)\) does indeed accurately predict the instantaneous covariance matrix of the oscillations.
        In particular, we can verify that each eigenvalue of  \(\Sigma(t)\) accurately predicts the variance of the oscillations along the corresponding eigenvector of \(\Sigma(t)\).
        (We actually use <em>generalized</em> eigenvectors/eigenvalues under the preconditioner \(P(t)\).)
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%"">
                <source src="../media/rmsprop-predict-sigma.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">Each generalized eigenvalue of \(\Sigma(t)\) w.r.t the preconditioner \(P(t)\) accurately predicts the variance of the oscillations along the corresponding generalized eigenvector of \(\Sigma(t)\).</div>
    </div>
    <p class="body-text">
        Thus, while the exact oscillations may be chaotic and hard to analyze, their time-average is predictable and comparably easy to analyze.
    </p>
    <h2>Interpreting the central flow</h2>
    <p class="body-text">
        We will now use the central flow to understand how RMSProp implicitly sets its preconditioner.
    </p>
    <p class="body-text">
        First, let us revisit the fluctuations in the squared gradient \(\nabla L(w_t)^{\odot 2} \) that we saw earlier.
        These fluctuations arise due to the oscillations in weight space along the top eigenvectors of the effective Hessian.
        Since the central flow models the covariance \(\Sigma(t)\) of these oscillations, it can render predictions for the time-average of the squared gradient, \( \mathbb{E}[\nabla L(w_t)^{\odot 2}] \),  via equation \eqref{eq:time-average-sq-gradient}.
        As you can see, this prediction is quite accurate:
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%"">
                <source src="../media/rmsprop-central-flow-sq-gradient.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">Using equation \eqref{eq:time-average-sq-gradient}, the central flow can accurately predict the time-average of the squared gradient.</div>
    </div>
    <p class="body-text">
        Thus, while the precise fluctuations in the squared gradient may be chaotic, their time-average is predictable.
    </p>
    <p class="body-text">
        Interestingly, we can see that most of the squared gradient \(\nabla L(w_t)^{\odot 2} \) originates from the oscillations, i.e. the second term of \eqref{eq:time-average-sq-gradient}, rather than from the squared gradient at the time-averaged iterate, i.e. the first term of \eqref{eq:time-average-sq-gradient}.
        Intuitively, if we recall that an oscillating optimization algorithm can be visualized as moving through a "valley" while bouncing between the "valley walls", then
        this means that most of the squared gradient comes from the valley walls, rather than from the valley floor.
    </p>
    <p class="body-text">
        The central flow can also accurately predict the dynamics of \(\nu\), the EMA of the squared gradient:
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%"">
                <source src="../media/rmsprop-central-flow-nu.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">The central flow's \(\nu(t)\) accurately models RMSProp's \(\nu_t\).</div>
    </div>
    <p class="body-text">
        In keeping with the broader story, we see that although \(\nu_t\) is rapidly fluctuating, its time-averaged trajectory \(\nu(t)\) is smooth and predictable.
    </p>

    <p class="body-text">
        Let us now use the central flow to understand how RMSProp sets its preconditioner.
        <!-- As is, the RMSProp central flow is a joint flow over both \((w, \nu)\). -->
        Unfortunately, even at the edge of stability, \(\nu(t)\) cannot always be expressed purely as a function of \(w(t)\) (as it was for Scalar RMSProp in <a href="../part2">Part II</a>), 
        and instead remains an independent variable that must be tracked.
        This reflects the fact that for any \(w\), there are potentially many values for \(\nu\) that could stabilize optimization, and the actual value used by RMSProp depends on the historical trajectory.
     </p>

    <p class="body-text">
        Nevertheless, it turns out that, in some circumstances, \(\nu\) implicitly converges under the dynamics of RMSProp to a value that depends on the current \(w\) alone.
        For any weights \(w\), imagine freezing \(w\) in place and running the \(\nu\) dynamics until a fixed point is reached.
        It turns out that for any \(w\), there is always a unique \(\nu\) satisfying the stationarity condition
        \(
        \frac{d\nu}{dt} = 0
        \).
        We refer to this unique \(\nu\) as the <em>stationary EMA</em>, denoted \( \overline{\nu}(w) \), for the weights \(w\).
        We will say more about the stationary EMA momentarily, but first let us verify the relevance of the stationarity argument.
    </p>

    <p class="body-text">
        In the figure below, we measure the cosine similarity between the real EMA \(\nu(t)\) and the stationary EMA \(\overline{\nu}(w(t))\) during training by the central flow.
        You can see that the cosine similarity rises to very high values (near 1), indicating that the real EMA \(\nu(t)\) converges to the stationary EMA \(\overline{\nu}(w(t))\).
    </p>

    <div class="figure">
        <img src="../media/stationary-nu-convergence-cosine.png" style="width:50%; display: block; margin: 0 auto;" alt="Alt text">
        <div class="caption">The cosine similarity between the real EMA \(\nu(t)\) and the stationary EMA \( \overline{\nu}(w(t)) \) rises to nearly 1.</div>
    </div>

    <p class="body-text">
        We can also see that the real EMA \(\nu(t)\) agrees with the stationary EMA \(\overline{\nu}(w(t))\) on a coordinate-wise level:
    </p>

    <div class="figure">
        <img src="../media/stationary-nu-convergence-coordinatewise.png" style="width:70%; display: block; margin: 0 auto;" alt="Alt text">
        <div class="caption">The real EMA \( \nu(t) \) (dots) matches the stationary EMA \(\overline{\nu}(w(t))\) (lines) coordinate-wise.  Ten coordinates are shown (colors).</div>
    </div>

    <p class="body-text">
        Intuitively, the original central flow describes the simultaneous dynamics of optimization (the \(w\) dynamics), and preconditioner adaptation (the \(\nu\) dynamics).
        That \(\nu(t)\) is converging to \(\overline{\nu}(w(t))\) implies that the \(\nu\) dynamics of preconditioner adaptation 
        are happening on a <em>faster timescale</em> than the \(w\) dynamics of optimization, so that the adaptive preconditioner is usually "all caught up" with the current weights \(w\).
        <!-- This means that we can examine the stationary EMA \(\overline{\nu}(w)\) to reason about RMSProp's preconditioning strategy. -->
    </p>

    <p class="body-text">
        So, what <em>is</em> the stationary EMA \(\overline{\nu}(w)\)?
        It is a bit analytically nicer to work in terms of the corresponding <em>stationary preconditioner</em> \( \overline{P}(w) := \frac{\sqrt{\overline{\nu}(w)}}{\eta} \).
        Remarkably, one can show that this stationary preconditioner is the optimal solution to a convex optimization problem over preconditioners:
    </p>
    <p class="body-text">
        \[
        \begin{align}
        \overline{P}(w) = \operatorname*{argmin}_{\substack{P \text{ diagonal}, \; P \succeq 0}} \ \text{trace}(P) + \tfrac{1}{\eta^2} \underbrace{\| \nabla L(w)\|^2_{P^{-1}}}_{\text{optimization speed}} \quad \text{such that} \quad \underbrace{H(w) \preceq 2P}_{\text{local stability}}. \label{eq:convex-program-P}
        \end{align}
        \]
    </p>
    <p class="body-text">
        That is, <strong>RMSProp implicitly solves the convex program \eqref{eq:convex-program-P} to compute its preconditioner</strong>.
        This behavior is <em>implicit</em> in the optimizer's oscillatory dynamics.
        Our analysis has rendered RMSProp's preconditioning strategy <em>explicit</em> for the first time.
    </p>

    <p class="body-text">
        Now that we are able to write down the preconditioner that RMSProp implicitly uses, we can interpret this preconditioner to gain some understanding
        into RMSProp's preconditioning strategy.
        The constraint in \eqref{eq:convex-program-P} is equivalent to the stability condition \( P^{-1} H(w) \preceq 2 I \) and hence ensures that the preconditioner keeps RMSProp locally stable.
        The first term in the objective of \eqref{eq:convex-program-P} is the trace of the preconditioner, or equivalently the sum of the inverse effective learning rates.
        If this were the only term in the objective, then the optimization problem \eqref{eq:convex-program-P} would reduce to maximizing the sum of the inverse effective learning rates while maintaining local stability:
    </p>

    <p class="body-text">
        \[
        \begin{align}
        \hat{P}(w) := \operatorname*{argmin}_{\substack{P \text{ diagonal}, \; P \succeq 0}} \ \text{trace}(P) \quad \text{such that} \quad \underbrace{H(w) \preceq 2P}_{\text{local stability}}. \label{eq:convex-program-P-alternate}
        \end{align}
        \]
    </p>
    <p class="body-text">
        <!-- Since the sum of the inverse effective learning rates is proportional to the <em>harmonic mean</em> of the effective learning rates, the problem \eqref{eq:convex-program-P-alternate} is equivalent to -->
        <!-- maximizing the harmonic mean of the effective learning rates while maintaining local stability. -->
        This is an intuitively sensible preconditioner criterion.
        What do we want in a preconditioner?  Well, we want the effective learning rates to be as large as possible, but we also need local stability, and these two desiderata are in tension.
        To resolve the tension, if we have some scalar-valued metric for quantifying the "largeness" of the vector of effective learning rates, then we can formulate preconditioner selection as a constrained optimization problem.
        Minimizing the sum of the inverse effective learning rates (i.e the trace of the preconditioner) is one such metric.
        <!-- The harmonic mean is one such metric. -->
    </p>
    <p class="body-text">
        In fact, if the diagonal constraint were not present, and if the Hessian \(H(w)\) were PSD, then we could say more: the optimization problem eq. \eqref{eq:convex-program-P-alternate} would have the closed-form solution \( \hat{P}(w) = \tfrac{1}{2} H(w) \).
        That is, the preconditioner \(P\) would be a scaling of the Hessian, and preconditioned gradient descent would move in the same direction as Newton's method.
        Even if the Hessian were not PSD, a similar point would hold &mdash; the optimization problem eq. \eqref{eq:convex-program-P-alternate} would have the closed-form solution \( \hat{P}(w) = \tfrac{1}{2} \Pi_{\mathbb{S}_+} H(w) \), where \( \Pi_{\mathbb{S}_+} \) denotes projection onto the cone of positive semidefinite matrices.
    </p>
    <p class="body-text">
        With the diagonal constraint present, this argument no longer applies, but we can empirically assess the efficacy of the preconditioner eq. \eqref{eq:convex-program-P-alternate} on deep learning loss functions.
        To quantify the efficacy of a preconditioner \(P\) at weights \(w\), we can measure the quantity \( \| \nabla L(w) \|^2_{P^{-1}} := \nabla L(w)^T P^{-1} \nabla L(w) \), which is the instantaneous rate of loss decrease
        under the preconditioned gradient flow.
        A natural "baseline" preconditioner is the one corresponding to vanilla gradient descent with the maximum locally stable learning rate, i.e. \(P(w) =  (2/S(w))^{-1} I\), where \(S(w)\) is the largest Hessian eigenvalue at \(w\).
    </p>
    <p class="body-text">
        The figure below shows that, along the RMSProp central flow at various learning rates, the preconditioner eq. \eqref{eq:convex-program-P-alternate} indeed beats this baseline.
    </p>
    <div class="figure">
        <img src="../media/optimization-speeds-wo-stationary.png" style="width:100%; display: block; margin: 0 auto;" alt="Alt text">
        <div class="caption">Along the central flow at various learning rates (subplots), the preconditioner \( \hat{P}(w) \) defined by eq. \eqref{eq:convex-program-P-alternate} (orange) strongly outperforms the "baseline" of \( P = (2/S(w))^{-1} I \) (green),
          which corresponds to gradient descent with the largest locally stable step size.
        </div> 
    </div>

    <p class="body-text">
        Unfortunately, the second term in the objective of \eqref{eq:convex-program-P} makes matters more complicated.
        This term is precisely the optimization speed of preconditioned gradient flow with preconditioner \(P\).
        <em>Minimizing</em> this quantity necessarily acts to <em>slow down</em> optimization, which is bad.
        Indeed, the figure below shows that RMSProp's stationary preconditioner \eqref{eq:convex-program-P} is always worse than the first-term-only variant \eqref{eq:convex-program-P-alternate}, and is sometimes even worse than our "baseline" of gradient descent:
    </p>

    <div class="figure">
        <img src="../media/optimization-speeds.png" style="width:100%; display: block; margin: 0 auto;" alt="Alt text">
        <div class="caption">The stationary preconditioner \(\overline{P}\)(w) given by eq. \eqref{eq:convex-program-P} (blue) is always worse than the first-term-only variant given by eq. \eqref{eq:convex-program-P-alternate} (orange)
            and is sometimes worse than the gradient descent baseline (green). </div> 
    </div>
    <p class="body-text">
        Thus, the second term in eq. \eqref{eq:convex-program-P} is a harmful presence in RMSProp's implicit preconditioning strategy.
    </p>
    <p class="body-text">
        In summary, our analysis has rendered <em>explicit</em> the preconditioner eq. \eqref{eq:convex-program-P} that is <em>implicit</em> in RMSProp's oscillatory dynamics.
        We are not claiming that this preconditioner is optimal in any sense; all we are claiming that this is exactly what RMSProp does, for better or for worse.
        We believe that understanding this preconditioning strategy is likely a prerequisite for understanding many other aspects of RMSProp's behavior.
    </p>
    <p class="body-text">
        So, is that it?  Should we think of RMSProp as an algorithm that implicitly performs preconditioned gradient descent using the preconditioner eq. \eqref{eq:convex-program-P}?
        Not quite &mdash; there is one thing missing from this picture.
        RMSProp is oscillating, and while much of the oscillatory motion cancels out, the oscillations induce a curvature reduction effect which we must account for.
        To do so, we can return to the central flow.
    </p>
    <p class="body-text">
        By substituting the stationary preconditioner \eqref{eq:convex-program-P} into the central flow, we can obtain a <em>stationary flow</em> over \(w\) alone:
        \[
        \begin{align}
        \frac{dw}{dt} = - \underset{\color{red} \substack{\text{stationary} \\ \text{preconditioner}}}{\overline{P}(w)^{-1}} \odot \left[\nabla L(w) +
         \underset{ \color{red} \text{implicit curvature penalty} }{\tfrac{1}{2} \nabla \langle \Sigma(t), H(w) \rangle}  \right], \label{eq:stationary-flow}
        \end{align}
        \]
    </p>
    <p class="body-text">
        where \(\Sigma(t)\) is defined as the solution to an SDCP.
        This flow models the preconditioner as adapting "infinitely fast" to the current weights \(w\), so that we can treat it as always being fixed at its current stationary value.
        It suggests that we should view RMSProp as performing preconditioned gradient descent, using the preconditioner eq. \eqref{eq:convex-program-P}, not on the original objective but on a certain curvature-penalized objective.
    </p>
    <p class="body-text">
        Empirically, we find that the stationary flow \eqref{eq:stationary-flow} is less accurate than the original central flow, but is nevertheless often a decent approximation to the RMSProp trajectory, provided that 
        RMSProp has reached the edge of stability and that the preconditioner has converged to its stationary value.
    </p>
    </p>
    <p class="body-text">
        As with Scalar RMSProp (<a href="../part2">Part II</a>), we find that the implicit curvature penalty is beneficial for RMSProp's efficacy as an <em>optimizer</em> (i.e. even if we don't care about generalization, and care only about making the loss go down fast).
        In particular, if we disable the central flow's implicit curvature penalty, then we find that the flow proceeds into higher-curvature regions in which it takes smaller steps, and optimizes slower:
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%"">
                <source src="../media/rmsprop-acc-via-reg.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">We compare RMSProp (blue) and its central flow (black) against an ablated flow (red) where the implicit curvature regularizer is disabled.
            Over time, this ablated flow navigates into higher-sharpness regions (center), where it takes smaller steps (right) and optimizes slower (left) than RMSProp.</div>
    </div>
    <p class="body-text">
        Indeed, in the limit of large \(\eta\), the stationary preconditioner eq. \eqref{eq:convex-program-P} converges to eq. \eqref{eq:convex-program-P-alternate}, which is independent of \(\eta\).
        Thus, in this limit, the learning rate hyperparameter \(\eta\) has no direct effect on the stationary preconditioner, and only affects the stationary flow via the curvature regularization term.
        But, as with Scalar RMSProp (<a href="../part2">Part II</a>), this can give \(\eta\) an <em>indirect</em> effect on the preconditioner later in training.
    </p>
    <!-- <h2>Conclusion</h2>

    <p class="body-text">
        Adam and RMSProp defy intuition from traditional optimization theory.
        Whereas a good preconditioner is supposed to adapt to the local <em>curvature</em> (i.e. Hessian), these optimizers instead update their preconditioner using the squared <em>gradient</em>.
        Motivated by this apparent disconnect between theory and practice, many works have tried to dethrone Adam by designing optimizers that explicitly use Hessian information.
        Yet these Hessian-aware approaches have fallen by the wayside over time, while Adam remains popular with practitioners.
    </p>

    <p class="body-text">
        Our analysis takes a step towards explaining this state of affairs.
        We have shown that RMSProp (i.e. the simplest special case of Adam) <em>does</em> adapt to the local Hessian after all &mdash; but the reason is inextricably tied to the algorithm's oscillatory dynamics, which haven't been previously studied.
        In short, the local quadratic structure of the loss function drives RMSProp to oscillate.
        These oscillations cause the gradient to implicitly pick up Hessian information.
        Although the precise dynamics are challenging to analyze and are likely chaotic, the covariance of the oscillations is predictable, and therefore so is the induced preconditioner, which we make explicit as eq. \eqref{eq:convex-program-P}.
    </p>
    <p class="body-text">
        Thus, for the first time, we now understand arguably the most basic aspects of RMSProp's behavior.
        We think that this knowledge is probably a prerequisite for comprehending other aspects of RMSProp's (and Adam's) behavior.
        Moreover, our analysis suggests two principles that one should keep in mind when designing new optimizers.
    </p>
    <p class="body-text">
        Thus, we see that an algorithm that only accesses the loss via <em>gradients</em> is able to implicitly adapt to the local Hessian.
        (While Quasi-Newton methods are broadly similar in spirit, the details are all different.)
        Notably, RMSProp does not need to compute any Hessian-vector products; it has no computational overhead beyond gradient descent.
    </p>



    
    <p class="body-text">
        To summarize, RMSProp takes an oscillatory path through weight space, oscillating along the top eigenvectors of the effective Hessian.
        These oscillations dominate the squared gradient and \(\nu\), and hence largely determine the preconditioner \( P_t = \text{diag}(\sqrt{\nu} / \eta) \).
        Indeed, it is not an exaggeration to say that RMSProp's preconditioner is <em>implicit</em> in the algorithm's oscillatory dynamics.
    </p>

    <p class="body-text">
        Although the oscillatory dynamics are complex and may be chaotic, the covariance of the oscillations is predictable, and this lets us solve for the time-averaged trajectory of both the weights \(w(t)\) and the EMA \(\nu(t)\).
        Empirically, the dynamics of \(\nu(t)\) seem to be happening on a faster timescale than the dynamics of \(w(t)\), so that \(\nu(t)\) is usually located right at its fixed point for the current weights \(w(t)\).
        Thus, we can view this fixed point as "the RMSProp preconditioner" that is implicitly used by RMSProp at weights \(w\).
    </p>

    <p class="body-text">Finally, the oscillations also induce implicit reduction of curvature, and this effect is crucial for the efficacy of RMSProp as an optimizer: 
        implicitly regularizing curvature allows RMSProp to take larger steps later in training.
    </p> -->

    <p class="companion-text">
        Interested in this line of work?  Consider pursuing a PhD with <a href="https://web.math.princeton.edu/~ad27/">Alex Damian</a>, who will join the MIT Math and EECS departments in Fall 2026.
     </p>
</body>
</html> 