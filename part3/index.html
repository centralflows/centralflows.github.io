<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How does RMSProp work?</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <link rel="stylesheet" href="../styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="../js/mathjax-config.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../js/video-controls.js"></script>
</head>
<body>
    <nav class="top-nav">
        <div class="top-nav-left">
            <a href="https://arxiv.org/abs/2410.24206">ðŸ“„ Paper</a>
        </div>
        <div class="top-nav-center">
            <a href="../">Introduction</a>
            <span class="separator">|</span>
            <a href="../part1">Part I</a>
            <span class="separator">|</span>
            <a href="../part2">Part II</a>
            <span class="separator">|</span>
            <a href="../part3" class="active">Part III</a>
            <span class="separator">|</span>
            <a href="../conclusion">Conclusion</a>
        </div>
        <div class="top-nav-right">
            <a href="https://github.com/locuslab/central_flows">ðŸ’» Code</a>
        </div>
    </nav>

    <p class="companion-text">
        This is the companion website for the paper <a href="https://arxiv.org/abs/2410.24206">Understanding Optimization in Deep Learning with Central Flows</a>, published at ICLR 2025.
     </p>

    <h1>Part III: how does RMSProp work?</h1>

    <p class="body-text">
        We now study <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">RMSProp</a>, which is equivalent to Adam without momentum:
        \[
            \underset{\color{red}{\text{update EMA of squared gradient}\,\rule[18pt]{0pt}{0pt}}}
                     {\nu_{t} = \beta_{2} \, \nu_{t-1} + (1-\beta_{2}) \nabla L(w_t)^{\odot 2}}, \quad \quad
            \underset{\color{red}{\text{take preconditioned gradient step}}}{w_{t+1} = w_t - \frac{\eta}{\sqrt{\nu_{t}} }\odot \nabla L(w_t)}.
        \]
    </p>
    <p class="body-text">
        RMSProp maintains an exponential moving average \(\nu\) of the element-wise squared gradient \(\nabla L(w)^{\odot 2}\), and then takes a step using per-coordinate step sizes of \(\eta / \sqrt{\nu}\).
        There is an equivalent interpretation that is sometimes more convenient to work with: if we define preconditioned gradient descent as: \( w_{t+1} = w_t - P^{-1} \nabla L(w_t)\),
        then RMSProp is preconditioned gradient descent with the dynamic preconditioner \(P_t = \text{diag}(\sqrt{\nu_t} / \eta)\).
    </p>
    <p class="body-text">
        <a href="https://arxiv.org/abs/1412.6980">Adam</a> uses the same preconditioning strategy, and has achieved massive success in deep learning.  However, it is a <a href="https://parameterfree.com/2020/12/06/neural-network-maybe-evolved-to-make-adam-the-best-optimize">mystery</a> why this particular preconditioner should work so well.
        Common folklore is that Adam/RMSProp adapts to the "curvature", i.e. Hessian, of the objective function.
        However, it is unclear why this should be the case, since Adam/RMSProp updates its preconditioner using the <em>gradient</em> \( \nabla L \), not the Hessian \( \nabla^2 L \).
    </p>
    <p class="body-text">
        In this section, we will use the central flows framework to understand RMSProp.
        We will see that RMSProp <em>does</em> adapt to the local Hessian after all &mdash; but the reason is inextricably tied to the optimizer's oscillatory dynamics, which have not been previously studied.
    </p>
    <h2>The dynamics of RMSProp</h2>
    <p class="body-text">
        Let's start by getting some intuition for RMSProp's behavior.
        We'll train a ResNet on a subset of CIFAR-10 using deterministic RMSProp.
        As you can see, the loss curve looks pretty well-behaved:
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%" style="width: 40%; display: block; margin: 0 auto;">
                <source src="../media/rmsprop-loss.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">We train a ResNet on a subset of CIFAR-10 using deterministic RMSProp.</div>
    </div>
    <p class="body-text">
        Yet, even though it looks like smooth sailing, there are interesting things happening beneath the surface.  Recall that RMSProp maintains an EMA \( \nu_t\) of the element-wise squared gradient  \( \nabla L(w_t)^{\odot2} \).
        Let's examine the dynamics of these two quantities at several individual coordinates:
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%"">
                <source src="../media/rmsprop-discrete-nu.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">The individual entries of the squared gradient (faint lines) fluctuate rapidly, causing their EMA (dark lines) to also fluctuate.</div>
    </div>
    <p class="body-text">
        You can see that the entries of the squared gradient \( \nabla L(w_t)^{\odot 2} \) are fluctuating rapidly.  This causes their EMA \( \nu_t \) to also fluctuate (although less so).
        Since this EMA is used to determine the algorithm's step sizes \( \eta / \sqrt{\nu_t} \), we need to understand the origin of this behavior if we want to understand how RMSProp sets its step sizes / preconditioner.
    </p>
    <p class="body-text">
        These fluctuations in the gradient arise because, like gradient descent, RMSProp is operating in an oscillatory <em>edge of stability</em> regime.
    </p>
    <p class="body-text">
        To understand why RMSProp would oscillate, consider running preconditioned gradient descent: \[ w_{t+1} = w_t - P^{-1} \nabla L(w_t) \] on a quadratic function with Hessian \( H \).
        If the preconditioned Hessian \( P^{-1} H \) has any eigenvalue greater than 2, then preconditioned gradient descent will oscillate along the corresponding (right) eigenvector of the preconditioned Hessian, as illustrated in the animation below.
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%" style="width: 40%; display: block; margin: 0 auto;">
                <source src="../media/preconditioned-gd-quadratic.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">We run preconditioned gradient descent on a quadratic function \(\frac{1}{2} w^T H w\), using a preconditioner \(P\) for which \(\lambda_\max(P^{-1} H) > 2\). 
            The right eigenvectors of the preconditioned Hessian \(P^{-1} H \) are shown as \( \mathbf{q}_1 \) and \( \mathbf{q}_2 \).
            Observe that the optimizer diverges along the direction \( \mathbf{q}_1 \), not the top Hessian eigenvector (which can be visualized as the top principal axis of the quadratic).
        </div>
    </div>
    <p class="body-text">
        While deep learning objectives are not globally quadratic, a <em>local</em> quadratic Taylor approximation suggests that RMSProp will oscillate
         if any eigenvalues of the <em>current</em> preconditioned Hessian \( P_t^{-1} H(w_t) = \text{diag}(\eta / \sqrt{\nu_t}) \, H(w_t) \) exceed the critical threshold 2.
    </p>
    <p class="body-text">
        Let's examine how the eigenvalues of the preconditioned Hessian evolve, under the dynamics of RMSProp:
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%" style="width: 80%; display: block; margin: 0 auto;">
                <source src="../media/rmsprop-eos.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">As RMSProp trains, the top eigenvalue of the preconditioned Hessian \( \text{diag}(\eta / \sqrt{\nu}) H(w_t)\) equilibrates around the value 2.</div>
    </div>
    <p class="body-text">
        Initially, all eigenvalues are below 2, and the optimizer does not oscillate.
        Once the top eigenvalue rises to the threshold 2, RMSProp enters an <em>edge of stability</em> regime where optimizer oscillates along the unstable eigenvector(s) of the preconditioned Hessian, and such oscillations induce reduction of the corresponding eigenvalue(s).
        The net effect is that the top eigenvalues equilibrate around the value 2.
    </p>
    <p class="body-text">
        For RMSProp, there are two distinct mechanisms by which oscillations along the unstable eigenvectors reduce the unstable eigenvalues of the preconditioned Hessian \( P_t^{-1} H(w_t) \).
        First, such oscillations induce implicit regularization of the Hessian \(H(w_t)\), as can be seen by considering a local cubic Taylor expansion.
        Second, they increase the gradient (as will be made precise below) and hence the preconditioner \(P_t\).
    </p>
    <p class="body-text">
        While analyzing the exact dynamics is challenging, analyzing the <em>time-averaged</em> dynamics is much easier.
        We will now derive a central flow \(w(t)\) that models the time-averaged trajectory of RMSProp.
    </p>
    <h2>Deriving the central flow</h2>
    <div class="figure" style="width:30%; float: right; margin: 0 0 16px 32px;">
        <img src="../media/divergence-illustration-flow-delta.png">
        <div class="caption">The iterate \(w_t\) is displaced from the central flow \(w(t)\) by some perturbation \(\delta_t\)</div>
    </div>
    <p class="body-text">
        As in <a href="../part1">Part I</a>, we model RMSProp \(\{w_t\}\) as being displaced from the central flow \(w(t)\) by some perturbation \(\delta_t\) with covariance \(\Sigma(t)\) that lies
        within the span of the unstable eigenvectors.
        \[
            \underset{\color{red} \text{iterate}}{w_t} = \underset{\color{red} \begin{array}{c}  \text{time-averaged} \\[-4pt] \text{iterate} \end{array} }{w(t)} + \underset{\color{red} \begin{array}{c} \text{perturbation} \end{array} }{\delta_t}  \quad \text{where} \quad \mathbb{E}[\delta_t \delta_t^T] = \Sigma(t).
        \]
    </p>
    <p class="body-text">
        Recall from <a href="../part1">Part I</a> that the time-averaged gradient is then approximately:
        \[
        \underbrace{\mathbb{E}[\nabla L(w_t)]}_{\color{red}{\substack{\text{time-average} \\ \text{of gradient}}}} \approx
             \underbrace{\nabla L(w(t))}_{\color{red}{\substack{\text{gradient at time-} \\ \text{averaged iterate}}}}
             + \underbrace{\tfrac{1}{2} \nabla \langle \Sigma(t), H(w(t)) \rangle}_{\color{red}{\substack{\text{implicit curvature} \\ \text{reduction}}}} 
            
        \]
    </p>
    <p class="body-text">
        We can similarly compute the time-average of the squared gradient.
        By using the first two terms in the Taylor expansion of the gradient, and by utilizing the fact that \(\delta_t\) is contained within the right eigenspace of the preconditioned Hessian that corresponds to the eigenvalue 2, we have:
        \[
        \begin{align}
        \underbrace{\mathbb{E}[\nabla L(w_t)^{\odot 2}]}_{\color{red}{\substack{\text{time-average of} \\ \text{ squared gradient}}}}
        \approx \underbrace{\nabla L(w(t))^{\odot 2}}_{\color{red}{\substack{\text{squared gradient at} \\ \text{time-averaged iterate}}}} + \underbrace{\frac{4\nu(t)}{\eta^2} \odot \text{diag}[\Sigma(t)]}_{\color{red}{\substack{\text{contribution from oscillations}}}}. \label{eq:time-average-sq-gradient}
        \end{align}
        \]
    </p>
    <p class="body-text">
        This calculation makes clear that larger oscillations (i.e. larger diagonal entries of \(\Sigma(t)\)) <em>increase</em> the squared gradient on average over time.
    </p>

    <p class="body-text">
        Based on these time-averages, we make the ansatz that the time-averaged dynamics of RMSProp can be described by a central flow \((w(t), \nu(t)) \) of the following functional form:
        \[
            \begin{align}
                \frac{dw}{dt} = -\frac{\eta}{\sqrt{\nu}} \odot \left[\underbrace{\nabla L(w) + \tfrac{1}{2} \nabla \langle \Sigma(t), H(w) \rangle}_{\color{red} \text{time-average of gradient} }\right], \quad \quad
                \frac{d\nu}{dt} = \frac{1-\beta_2}{\beta_2} \left[\underbrace{\nabla L(w)^{\odot 2} + \frac{4\nu}{\eta^2} \odot \text{diag}[\Sigma(t)]}_{\color{red} \text{time-average of squared gradient} } - \nu \right ].
                \label{eq:ansatz}
            \end{align}
        \]
        As before, to determine \(\Sigma(t)\), we impose three natural conditions, and show that only one value of \(\Sigma(t)\) satisfies all three conditions.
        This value is the solution to a certain semidefinite complementarity problem (SDCP).
        The RMSProp central flow is defined as equation \eqref{eq:ansatz} with this value of \(\Sigma(t)\).
    </p>
    
    <p class="body-text">
        Let's take a look at the central flow in action.
        You can see that under the central flow, each eigenvalue of the preconditioned Hessian rises to 2 and then stays locked at this value:
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%" style="width: 80%; display: block; margin: 0 auto;">
                <source src="../media/rmsprop-centralflow.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">Under the central flow, each eigenvalue of the preconditioned Hessian rises to 2 and then stays locked at this value.
        </div>
    </div>
    <p class="body-text">
        We can furthermore see that \(\Sigma(t)\) does indeed accurately predict the instantaneous covariance matrix of the oscillations.
        In particular, we can verify that each generalized eigenvalue of  \(\Sigma(t)\) w.r.t the preconditioner \(P(t)\) accurately predicts the variance of the oscillations along the corresponding generalized eigenvector of \(\Sigma(t)\).
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%"">
                <source src="../media/rmsprop-predict-sigma.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">Each generalized eigenvalue of \(\Sigma(t)\) w.r.t the preconditioner \(P(t)\) accurately predicts the variance of the oscillations along the corresponding generalized eigenvector of \(\Sigma(t)\).</div>
    </div>
    <h2>Interpreting the central flow</h2>
    <p class="body-text">
        We will now use the central flow to understand how RMSProp implicitly sets its preconditioner.
    </p>
    <p class="body-text">
        First, let us revisit the fluctuations in the squared gradient \(\nabla L(w_t)^{\odot 2} \) that we saw earlier.
        These fluctuations arise due to the oscillations in weight space along the top eigenvectors of the effective Hessian.
        Since the central flow models the covariance \(\Sigma(t)\) of these oscillations, it can render predictions for the time-average of the squared gradient, \( \mathbb{E}[\nabla L(w_t)^{\odot 2}] \),  via equation \eqref{eq:time-average-sq-gradient}.
        As you can see, this prediction is quite accurate:
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%"">
                <source src="../media/rmsprop-central-flow-sq-gradient.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">Using equation \eqref{eq:time-average-sq-gradient}, the central flow can accurately predict the time-average of the squared gradient.</div>
    </div>
    <p class="body-text">
        Thus, while the precise fluctuations in the squared gradient may be chaotic, their time-average is predictable.
    </p>
    <p class="body-text">
        Interestingly, we can see that most of the squared gradient \(\nabla L(w_t)^{\odot 2} \) originates from the oscillations, i.e. the second term of \eqref{eq:time-average-sq-gradient}, rather than from the squared gradient at the time-averaged iterate, i.e. the first term of \eqref{eq:time-average-sq-gradient}.
        Intuitively, if we recall that an oscillating optimization algorithm can be viewed as moving through a "valley" while bouncing between the "valley walls", then
        this means that most of the squared gradient comes from the valley walls, rather than from the valley floor.
    </p>
    <p class="body-text">
        The central flow can also accurately predict the dynamics of \(\nu\), the EMA of the squared gradient:
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%"">
                <source src="../media/rmsprop-central-flow-nu.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">The central flow's \(\nu(t)\) accurately models RMSProp's \(\nu_t\).</div>
    </div>
    <p class="body-text">
        In keeping with the broader story, we see that although \(\nu_t\) is fluctuating, its time-averaged trajectory \(\nu(t)\) is predictable.
    </p>

    <p class="body-text">
        Let us now use the central flow to understand how RMSProp sets its preconditioner.
        <!-- As is, the RMSProp central flow is a joint flow over both \((w, \nu)\). -->
        Unfortunately, even at the edge of stability, \(\nu(t)\) cannot always be expressed purely as a function of \(w(t)\) (as it was for Scalar RMSProp in <a href="../part2">Part II</a>), 
        and instead remains an independent variable that must be tracked.
        This reflects the fact that for any \(w\), there are potentially many values for \(\nu\) that could stabilize optimization, and the actual value used by RMSProp depends on the historical trajectory.
     </p>

    <p class="body-text">
        Nevertheless, it turns out that, in some circumstances, \(\nu\) implicitly converges under the dynamics of RMSProp to a value that depends on the current \(w\) alone.
        For any weights \(w\), imagine freezing \(w\) in place and running the \(\nu\) dynamics until a fixed point is reached.
        It turns out that for any \(w\), there is always a unique \(\nu\) satisfying the stationarity condition
        \(
        \frac{d\nu}{dt} = 0
        \).
        We refer to this unique \(\nu\) as the <em>stationary EMA</em>, denoted \( \overline{\nu}(w) \), for the weights \(w\).
        We will say more about the stationary EMA momentarily, but first let us verify the relevance of the stationarity argument.
    </p>

    <p class="body-text">
        In the figure below, we measure the cosine similarity between the real EMA \(\nu(t)\) and the stationary EMA \(\overline{\nu}(w(t))\) during training by the central flow.
        You can see that the cosine similarity rises to very high values (near 1), indicating that the real EMA \(\nu(t)\) converges to the stationary EMA \(\overline{\nu}(w(t))\).
    </p>

    <div class="figure">
        <img src="../media/stationary-nu-convergence-cosine.png" style="width:50%; display: block; margin: 0 auto;" alt="Alt text">
        <div class="caption">Caption.</div>
    </div>

    <p class="body-text">
        We can also see that after a bit of time, the real EMA \(\nu(t)\) starts agreeing with the stationary EMA \(\overline{\nu}(w(t))\) on a coordinate-wise level:
    </p>

    <div class="figure">
        <img src="../media/stationary-nu-convergence-coordinatewise.png" style="width:70%; display: block; margin: 0 auto;" alt="Alt text">
    </div>

    <p class="body-text">
        Intuitively, the original central flow describes the simultaneous dynamics of optimization (the \(w\) dynamics), and preconditioner adaptation (the \(\nu\) dynamics).
        That \(\nu(t)\) is converging to \(\overline{\nu}(w(t))\) implies that the \(\nu\) dynamics of preconditioner adaptation 
        are happening on a <em>faster timescale</em> than the \(w\) dynamics of optimization, so that the adaptive preconditioner is usually "all caught up" with the current weights \(w\).
        <!-- This means that we can examine the stationary EMA \(\overline{\nu}(w)\) to reason about RMSProp's preconditioning strategy. -->
    </p>

    <p class="body-text">
        So, what <em>is</em> the stationary EMA \(\overline{\nu}(w)\)?
        It is a bit analytically nicer to work in terms of the corresponding <em>stationary preconditioner</em> \( \overline{P}(w) := \frac{\sqrt{\overline{\nu}(w)}}{\eta} \).
        Remarkably, one can show that this stationary preconditioner is the optimal solution to a convex optimization problem over preconditioners:
    </p>
    <p class="body-text">
        \[
        \begin{align}
        \overline{P}(w) = \operatorname*{argmin}_{\substack{P \text{ diagonal}, \; P \succeq 0}} \ \text{trace}(P) + \tfrac{1}{\eta^2} \underbrace{\| \nabla L(w)\|^2_{P^{-1}}}_{\text{optimization speed}} \quad \text{such that} \quad \underbrace{H(w) \preceq 2P}_{\text{local stability}}. \label{eq:convex-program-P}
        \end{align}
        \]
    </p>
    <p class="body-text">
        That is, <strong>RMSProp implicitly solves the convex program \eqref{eq:convex-program-P} to compute its preconditioner</strong>.
        This behavior is <em>implicit</em> in the optimizer's oscillatory dynamics.
        Our analysis has rendered RMSProp's preconditioning strategy <em>explicit</em> for the first time.
    </p>

    <p class="body-text">
        Now that we are able to write down the preconditioner that RMSProp implicitly uses, we can interpret this preconditioner to gain some understanding
        into RMSProp's efficacy.
        The constraint in \eqref{eq:convex-program-P} is equivalent to the stability condition \( P^{-1} H \preceq 2 I \) and hence ensures that the preconditioner keeps RMSProp locally stable.
        The first term in the objective of \eqref{eq:convex-program-P} is the trace of the preconditioner, or equivalently the sum of the inverse effective learning rates.
        If this were the only term in the objective, then the optimization problem \eqref{eq:convex-program-P} would reduce to maximizing the sum of the inverse effective learning rates while maintaining local stability:
    </p>

    <p class="body-text">
        \[
        \begin{align}
        \hat{P}(w) := \operatorname*{argmin}_{\substack{P \text{ diagonal}, \; P \succeq 0}} \ \text{trace}(P) \quad \text{such that} \quad \underbrace{H(w) \preceq 2P}_{\text{local stability}}. \label{eq:convex-program-P-alternate}
        \end{align}
        \]
    </p>
    <p class="body-text">
        Since the sum of the inverse effective learning rates is proportional to the <em>harmonic mean</em> of the effective learning rates, the problem \eqref{eq:convex-program-P-alternate} is equivalent to
        maximizing the harmonic mean of the effective learning rates while maintaining local stability.
        This is an intuitively sensible preconditioner criterion.
        What do we want in a preconditioner?  Well, we want the effective learning rates to be as large as possible, but we also need local stability, and these two desiderata are in tension.
        To resolve the tension, if we have some scalar-valued metric for measuring the "largeness" of the vector of effective learning rates, then we can formulate preconditioner selection as an optimization problem.
        The harmonic mean is one such metric.
    </p>

    <p class="body-text">
        Indeed, the figure below shows that, along the RMSProp central flow, the preconditioner \eqref{eq:convex-program-P-alternate} beats the "baseline" 
        of gradient descent with the largest locally stable learning rate, i.e. \(P(w) =  (2/S(w)))^{-1} I\).
        To measure the efficacy of a preconditioner \(P\) at weights \(w\), we report the quantity \[ \| \nabla L(w) \|^2_{P^{-1}} := \nabla L(w)^T P^{-1} \nabla L(w), \]
        which is the instantaneous rate of loss decrease under the preconditioned gradient flow with preconditioner \(P\).  Larger is better.
    </p>
    
    <div class="figure">
        <img src="../media/optimization-speeds-wo-stationary.png" style="width:100%; display: block; margin: 0 auto;" alt="Alt text">
    </div>

    <p class="body-text">
        However, the second term in the objective of \eqref{eq:convex-program-P} makes matters more complicated.
        This term is precisely the optimization speed of preconditioned gradient flow with preconditioner \(P\).
        <em>Minimizing</em> this quantity necessarily acts to <em>slow down</em> optimization, which is bad.
        Thus, this term is a harmful presence in RMSProp's implicit preconditioning strategy.
        Indeed, the figure below shows that RMSProp's stationary preconditioner \eqref{eq:convex-program-P} is always worse than the first-term-only variant \eqref{eq:convex-program-P-alternate}, and is sometimes even worse than our "baseline" of gradient descent:
    </p>

    <div class="figure">
        <img src="../media/optimization-speeds.png" style="width:100%; display: block; margin: 0 auto;" alt="Alt text">
    </div>

    <p class="body-text">
        By substituting the stationary preconditioner \eqref{eq:convex-program-P} into the RMSProp central flow, we can obtain a <em>stationary flow</em> in terms of \(w\) alone:
        \[
        \begin{align}
        \frac{dw}{dt} = - \underset{\color{red} \substack{\text{stationary} \\ \text{preconditioner}}}{\overline{P}(w)^{-1}} \odot \left[\nabla L(w) +
         \underset{ \color{red} \text{implicit curvature penalty} }{\tfrac{1}{2} \nabla \langle \Sigma(t), H(w) \rangle}  \right] \label{eq:stationary-flow}
        \end{align}
        \]
    </p>
    <p class="body-text">
        where \(\Sigma(t)\) is defined as the solution to an SDCP.
        This flow assumes that the preconditioner adapts "infinitely fast" to the current weights \(w\), so that we can model it as always being fixed at its current stationary value.
        Empirically, we find that the stationary flow \eqref{eq:stationary-flow} is less accurate than the original central flow, but is nevertheless often a decent approximation to the RMSProp trajectory, provided that 
        RMSProp has reached the edge of stability and that the preconditioner has converged to its stationary value.
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%"">
                <source src="../media/rmsprop-stationary-flow.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">this is currently a CNN; should switch to the resnet.</div>
    </div>
    <p class="body-text">
        As for Scalar RMSProp (<a href="../part2">Part II</a>), we find that the implicit curvature penalty is beneficial for RMSProp's efficacy as an <em>optimizer</em> (i.e. putting generalization considerations aside).
        In particular, if we disable the central flow's implicit curvature penalty, then we find that the optimizer proceeds into higher-curvature regions in which it takes smaller steps, and optimizes slower:
    </p>
    <div class="figure">
        <div class="video-container">
            <video width="100%"">
                <source src="../media/rmsprop-acc-via-reg.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <div class="video-overlay">Click to play</div>
        </div>
        <div class="caption">acceleration via regularization.</div>
    </div>
    <h2>Conclusion</h2>
    <p class="body-text">
        To summarize, RMSProp takes an oscillatory path through weight space, oscillating along the top eigenvectors of the preconditioned Hessian.
        These oscillations dominate the squared gradient and \(\nu\), and hence largely determine the preconditioner \( P_t = \text{diag}(\sqrt{\nu} / \eta) \).
        Indeed, it is not an exaggeration to say that RMSProp's preconditioner is <em>implicit</em> in the algorithm's oscillatory dynamics.
    </p>

    <p class="body-text">
        Although the oscillatory dynamics are complex and may be chaotic, the covariance of the oscillations is predictable, and this lets us solve for the time-averaged trajectory of both the weights \(w(t)\) and the EMA \(\nu(t)\).
        Empirically, the dynamics of \(\nu(t)\) seem to be happening on a faster timescale than the dynamics of \(w(t)\), so that \(\nu(t)\) is usually located right at its fixed point for the current weights \(w(t)\).
        Thus, we can view this fixed point as "the RMSProp preconditioner" that is implicitly used by RMSProp at weights \(w\).
    </p>

    <p class="body-text">Finally, the oscillations also induce implicit reduction of curvature, and this effect is crucial for the efficacy of RMSProp as an optimizer: 
        implicitly regularizing curvature allows RMSProp to take larger steps later in training.
    </p>

    <p class="companion-text">
        If you find this work interesting, consider applying to do a PhD with Alex Damian, who will join the MIT Math and EECS departments in Fall 2026.
     </p>
</body>
</html> 