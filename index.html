<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding Optimization in Deep Learning with Central Flows</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <link rel="stylesheet" href="styles.css">
    <style>
        /* Index-specific styles */
        .authors {
            text-align: center;
            margin-bottom: 32px;
            font-size: 22px;
        }
        .primary-authors {
            margin-bottom: 8px;
        }
        .author-name {
            margin: 0 12px;
        }
        .authors div:not(.equal-note) {
            margin-bottom: 8px;
        }
        .equal-note {
            font-size: 16px;
            margin-top: 20px;
            color: #444;
        }
        .figure {
            width: 60%;
            margin: 32px auto;
        }
        .btn {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            background: var(--blue);
            color: #fff;
            padding: 12px 24px;
            border-radius: 6px;
            font-weight: 500;
            transition: background 0.2s;
        }
        .btn:hover { 
            background: #2563EB; 
        }
        @media (max-width: 600px) {
            .authors {
                font-size: 20px;
            }
        }
    </style>
</head>
<body>    
    <nav class="top-nav">
        <a href="index.html" class="active">Introduction</a>
        <span class="separator">|</span>
        <a href="part1.html">Part I</a>
        <span class="separator">|</span>
        <a href="part2.html">Part II</a>
        <span class="separator">|</span>
        <a href="part3.html">Part III</a>
        <span class="separator">|</span>
        <a href="conclusion.html">Conclusion</a>
    </nav>

    <h1>Understanding optimization in deep learning with central flows</h1>

    <div class="authors">
        <div class="primary-authors">
            <span class="author-name"><a href="https://jmcohen.github.io">Jeremy Cohen*</a></span>
            <span class="author-name"><a href="https://web.math.princeton.edu/~ad27/">Alex Damian*</a></span>
        </div>
        <div>
            <span class="author-name"><a href="https://www.cs.cmu.edu/~atalwalk/">Ameet Talwalkar</a></span>
            <span class="author-name"><a href="https://zicokolter.com">J. Zico Kolter</a></span>
            <span class="author-name"><a href="https://jasondlee88.github.io/">Jason D. Lee</a></span>
        </div>
        <div class="equal-note">
            *Alex and Jeremy contributed equally; author order determined by coin flip
        </div>
    </div>

    <div class="links">
        <a href="https://arxiv.org/abs/2410.24206">ðŸ“„ Paper</a>
        <a href="https://github.com/locuslab/central_flows">ðŸ’» Code</a>
    </div>

    <div class="gif-container">
        <img src="https://jmcohen.github.io/images/overview.gif" alt="Central Flows Overview Animation">
    </div>
    <div class="caption">
        A vision transformer is trained on a subset of CIFAR-10 using gradient descent. The central flow (black) accurately matches the trajectory of gradient descent.
    </div>

    <p class="body-text">
        This is the companion website for the paper <a href="https://arxiv.org/abs/2410.24206">Understanding Optimization in Deep Learning with Central Flows</a>, published at ICLR 2025.
     </p>

    <p class="body-text">
        This work takes a step towards a <strong>theory of optimization in deep learning</strong>. 
        Traditional theories of optimization cannot describe the dynamics of optimization in deep learning, even in the simple setting of deterministic (i.e. full-batch) training.
        The challenge is that optimizers typically operate in a complex oscillatory regime termed the <a href="https://arxiv.org/abs/2103.00065">edge of stability.</a>
        In this work, we develop theory that can describe the dynamics of optimization in this regime.
     </p>

     <p class="body-text">
        Our key insight is that while the <em>exact</em> dynamics of an oscillatory optimizer may be challenging to analyze, the <em>time-averaged</em> (i.e. locally smoothed) dynamics are often much easier to understand.
        We characterize these dynamics with a <em>central flow</em>: a differential equation that directly models the time-averaged trajectory of an optimizer, as illustrated in the following cartoon.
     </p>

     <div class="figure">
        <img src="media/fig1.png", alt="Central flow cartoon">
    </div>
    <div class="caption">
        In this cartoon, gradient descent (blue) takes an oscillatory path through weight space. The central flow (black) is a smooth curve that characterizes this trajectory.
    </div>

    <p class="body-text">
        This site is organized as follows (paralleling different sections of the paper):
        <ul>
            <li>
                In <a href="part1.html">Part I</a>, we introduce the central flows framework with an analysis of <b>gradient descent</b>, the simplest optimizer.
            </li>
            <li> In <a href="part2.html">Part II</a>, we examine a simple adaptive optimizer, <b>Scalar RMSProp</b>.</li>
            <li>In <a href="part3.html">Part III</a>, we analyze <b>RMSProp</b>, i.e. Adam without momentum</li>
            <li>Finally, in the <a href="conclusion.html">Conclusion</a>, we review why central flows are a useful tool for understanding optimization in deep learning.</li>
        </ul>
    </p>

</body>
</html>
