<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Understanding Optimization in Deep Learning with Central Flows</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <link rel="stylesheet" href="../styles.css">
    <style>
        /* Index-specific styles */
        .authors {
            text-align: center;
            margin-bottom: 32px;
            font-size: 18px;
        }
        .primary-authors {
            margin-bottom: 8px;
        }
        .author-name {
            margin: 0 12px;
        }
        .authors div:not(.equal-note) {
            margin-bottom: 8px;
        }
        .equal-note {
            font-size: 16px;
            margin-top: 20px;
            color: #444;
        }
        .figure {
            width: 60%;
            margin: 32px auto;
        }
        .btn {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            background: var(--blue);
            color: #fff;
            padding: 12px 24px;
            border-radius: 6px;
            font-weight: 500;
            transition: background 0.2s;
        }
        .btn:hover { 
            background: #2563EB; 
        }
        @media (max-width: 600px) {
            .authors {
                font-size: 20px;
            }
        }
    </style>
</head>
<body>    
    <nav class="top-nav">
        <div class="top-nav-left">
            <a href="../central-flows-draft.pdf">ðŸ“„ Paper</a>
            <!-- <a href="https://arxiv.org/abs/2410.24206">ðŸ“„ Paper</a> -->
        </div>
        <div class="top-nav-center">
            <a href="../" class="active">Introduction</a>
            <span class="separator">|</span>
            <a href="../part1">Part I</a>
            <span class="separator">|</span>
            <a href="../part2">Part II</a>
            <span class="separator">|</span>
            <a href="../part3">Part III</a>
            <!-- <span class="separator">|</span>
            <a href="../conclusion">Conclusion</a> -->
        </div>
        <div class="top-nav-right">
            <a href="https://github.com/centralflows/centralflows">ðŸ’» Code</a>
        </div>
    </nav>
    
    <p class="companion-text">
        This is the companion website for the paper <a href="../central-flows-draft.pdf">Understanding Optimization in Deep Learning with Central Flows</a>, published at ICLR 2025.
        <!-- This is the companion website for the paper <a href="https://arxiv.org/abs/2410.24206">Understanding Optimization in Deep Learning with Central Flows</a>, published at ICLR 2025. -->
     </p>

    <h1>Understanding optimization in deep learning with central flows</h1>

    <div class="authors">
        <div class="primary-authors">
            <span class="author-name"><a href="https://jmcohen.github.io">Jeremy Cohen*</a></span>
            <span class="author-name"><a href="https://web.math.princeton.edu/~ad27/">Alex Damian*</a></span>
        </div>
        <div>
            <span class="author-name"><a href="https://www.cs.cmu.edu/~atalwalk/">Ameet Talwalkar</a></span>
            <span class="author-name"><a href="https://zicokolter.com">J. Zico Kolter</a></span>
            <span class="author-name"><a href="https://jasondlee88.github.io/">Jason D. Lee</a></span>
        </div>
        <div class="equal-note">
            *Alex and Jeremy contributed equally; author order determined by coin flip
        </div>
    </div>

    <!-- <div class="links">
        <a href="https://arxiv.org/abs/2410.24206">ðŸ“„ Paper</a>
        <a href="https://github.com/locuslab/central_flows">ðŸ’» Code</a>
    </div> -->

    <div class="gif-container">
        <img src="https://jmcohen.github.io/images/overview.gif" alt="Central Flows Overview Animation">
    </div>
    <div class="caption">
        A vision transformer is trained on a subset of CIFAR-10 using gradient descent. The central flow (black) accurately matches the trajectory of gradient descent.
    </div>

    <p class="body-text">
        This work takes a step towards a <strong>theory of optimization in deep learning</strong>. 
        Traditional theories of optimization cannot describe the dynamics of optimization in deep learning, even in the simple setting of deterministic (i.e. full-batch) training.
        The challenge is that optimizers typically operate in a complex oscillatory regime termed the <a href="https://arxiv.org/abs/2103.00065">edge of stability.</a>
        In this work, we develop theory that can describe the dynamics of optimization in this regime.
     </p>

     <p class="body-text">
        Our key insight is that while the <em>exact</em> dynamics of an oscillatory optimizer may be challenging to analyze, the <em>time-averaged</em> (i.e. locally smoothed) dynamics are often much easier to understand.
        We characterize these dynamics with a <em>central flow</em>: a differential equation that directly models the time-averaged trajectory of an oscillatory optimizer, as illustrated in the following cartoon.
     </p>

     <div class="figure">
        <img src="media/fig1.png", alt="Central flow cartoon">
    </div>
    <div class="caption">
        In this cartoon, gradient descent (blue) takes an oscillatory path through weight space. The central flow (black) is a smooth curve that characterizes this trajectory.
    </div>
    <p class="body-text">
        This site is organized as follows, paralleling different sections of the paper:
        <ul>
            <li>
                In <a href="../part1">Part I</a>, we analyze <b>gradient descent</b>, the simplest optimizer.
            </li>
            <li> In <a href="../part2">Part II</a>, we examine <b>Scalar RMSProp</b>, a simple adaptive optimizer.</li>
            <li>In <a href="../part3">Part III</a>, we study <b>RMSProp</b>, i.e. Adam without momentum.</li>
        </ul>
    </p>
    <p class="body-text">
        In the <a href="../conclusion">Conclusion</a>, we make the case that central flows are a necessary theoretical tool for reasoning about optimization in deep learning.
    </p>
    <p class="body-text">
        Our focus is on the simple setting of deterministic (i.e. full-batch) optimization, because even this simple setting is not yet understood by existing theory. 
        We regard our analysis of deterministic optimization as a necessary stepping stone to a future analysis of stochastic optimization.
    </p>
    <p class="body-text">
        Although our analysis is based on informal mathematical reasoning, we hold the resulting theory to the unusually high standard of rendering <strong>accurate numerical predictions</strong> about the optimization trajectories of <strong>real neural networks</strong>.
    </p>
    <p class="companion-text">
        Interested in this line of work?  Consider pursuing a PhD with <a href="https://web.math.princeton.edu/~ad27/">Alex Damian</a>, who will join the MIT Math and EECS departments in Fall 2026.
    </p>

</body>
</html>
