<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Conclusion: why central flows?</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <link rel="stylesheet" href="../styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="../js/mathjax-config.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../js/video-controls.js"></script>
</head>
<body>
    <nav class="top-nav">
        <div class="top-nav-left">
            <a href="https://arxiv.org/abs/2410.24206">ðŸ“„ Paper</a>
        </div>
        <div class="top-nav-center">
            <a href="../">Introduction</a>
            <span class="separator">|</span>
            <a href="../part1">Part I</a>
            <span class="separator">|</span>
            <a href="../part2">Part II</a>
            <span class="separator">|</span>
            <a href="../part3">Part III</a>
            <span class="separator">|</span>
            <a href="../conclusion" class="active">Conclusion</a>
        </div>
        <div class="top-nav-right">
            <a href="https://github.com/locuslab/central_flows">ðŸ’» Code</a>
        </div>
    </nav>

    <p class="companion-text">
        This is the companion website for the paper <a href="https://arxiv.org/abs/2410.24206">Understanding Optimization in Deep Learning with Central Flows</a>, published at ICLR 2025.
     </p>

    <h1>Conclusion: why central flows?</h1>

    <p class="body-text">
        Optimization is a core component of deep learning.
        <!-- Regardless of whether you're pre-training a large Transformer to predict the next token, "fine-tuning" this Transformer to become an assistant, or training a diffusion model, you are running an iterative optimization algorithm, -->
        If you are doing deep learning, you are running an iterative optimization algorithm,
        and there are certain optimization-related concerns that always come up:
        What does the learning rate do, and how should I set it?  Why is my gradient behaving like that? Why does one optimizer work better than another on a given task?  How can we build a better optimizer?
        A <strong>theory of optimization for deep learning</strong> would provide a <strong>language for reasoning</strong> about these questions.
    </p>
    <p class="body-text">
        Yet, there is currently no consensus within the community on what form this theory should take.
    In classical optimization theory, there are perhaps two prevalent styles of analysis, yet neither seems well-suited for deep learning.
    </p>
    <p class="body-text">
        First, there are local convergence analyses, which describe the optimizer's behavior in the neighborhood of a minimum.
        The problem with these is that deep learning mostly does not happen in the neighborhood of a minimum.
        In many cases, training terminates before anything remotely resembling a minimum is reached.
        Even if there are situations where training does eventually reach a minimum, the practitioner would care equally about the whole training process, and would not care disproportionately about the last moments.
    </p>
    <p class="body-text">
        Second, there are global convergence analyses, which aim to prove convergence to a minimum from any initialization.
        The problem with these is that, for real networks on real problems, nobody knows how to meaningfully characterize the rate of convergence of any optimizer.
        If we can't meaningfully characterize the rate for any single optimizer, then we can't meaningfully compare two optimizers by comparing their respective rates.
        <!-- It's possible that global convergence analyses only make sense in the context of convex optimization. -->
    </p>
    <p class="body-text">
        Another idea is to analyze optimization in "toy" settings, where the model and the dataset are simple enough for analysis to be tractable.
        But while meaningful results can often be proven about the toy model, it is never clear if or how such results would generalize to practical settings.
        For a practitioner training a real neural net on a real dataset, an analysis of a toy setting does not suggest anything concrete about the problem at hand.
    </p>
    <p class="body-text">
        We hope to advance the idea that the theory of optimization for deep learning could revolve around studying the <em>local dynamics</em> of the optimizer <em>throughout training</em> &mdash; that is, the question: "how does the optimizer move around the local loss landscape?"
        (We are certainly <a href="https://arxiv.org/abs/2009.11162">not</a> the <a href="https://arxiv.org/abs/2205.10287">first</a> to advocate for this idea.)
        In the setting of deterministic training, what makes these local dynamics interesting are <em>oscillations</em>.
        These oscillations emerge due to the interaction between the discrete update rule and the local loss landscape, and they induce rich dynamical behaviors.
        Indeed, it would not be an exaggeration to say that much of the behavior of an optimization algorithm is <em>implicit</em> in its oscillatory dynamics.
        <!-- As we've seen, without understanding these dynamics, it is not possible to understand gradient descent's mechanism of convergence, or how adaptive optimizers "adapt" to the local loss landscape. -->
    </p>
    <p class="body-text">
        Our means of analyzing the oscillatory dynamics of an optimization algorithm is to derive a <em>central flow</em> that directly models the optimizer's time-averaged, or smoothed, trajectory.
        This central flow renders <em>explicit</em> the behaviors that were previously <em>implicit</em> in the optimizer's oscillatory dynamics.
        It therefore provides a <b>strictly more informative representation</b> of the optimization algorithm than the "raw" update rule does.
    </p>
    <p class="body-text">
        To be sure, a central flow analysis does not reveal everything we'd like to know about an optimization algorithm.
        For example, our analysis does not allow us to characterize the global rate at which the training loss will decrease.
        For another example, although we say that optimizers implicitly follow a curvature-penalized gradient flow, we do not explain what such a curvature penalty means for learning.
        We regard these (and other) important questions as open problems for deep learning theory, beyond the scope of this work.
    </p>
    <ul>
        <li>
            <p class="body-text">
                Firstly, these local dynamics are more <em>interesting</em> than may have been assumed: we have seen that even vanilla gradient descent, the simplest optimizer, gives rise to surprisingly rich dynamics in deep learning.
            </p>
        </li>
        <li>
            <p class="body-text">
                Second, these local dynamics are <em>important</em>: we've shown that without understanding the local dynamics, it is not possible to understand gradient descent's mechanism of convergence, nor is it possible to understand how adaptive optimizers "adapt" to the local loss landscape.
            </p>
        </li>
        <li>
        <p class="body-text">
        Thirdly, these local dynamics are <em>generic</em>: once you look at things from the right angle, the picture looks exactly the same for any network trained on any dataset.
        We come up with nontrivial mathematical statements that seem to apply generically in deep learning.
        </p>
        </li>
    </ul>
    <p class="body-text">
        In the deterministic setting, which we study in this work, what makes the local dynamics interesting are <em>oscillations</em>.
        These oscillations induce rich dynamical behaviors.
        To analyze an optimizer, we derive a <em>central flow</em>: a smooth curve that explicitly characterizes the optimizer's time-averaged trajectory.
        The central flow renders <em>explicit</em> various behaviors that were previously <em>implicit</em> in the optimizer's oscillatory dynamics.
    </p>
    <p class="body-text">
        To be sure, our analysis leaves many questions unanswered.
        For example, we show that optimizers implicitly follow a curvature-penalized gradient flow, but we do not explain what such a curvature penalty means for learning.
        In our view, this is a concern for deep learning theory.
        We mean to achieve a separation of concerns between "optimization theory for deep learning" and "deep learning theory".
        Research in "optimization theory for deep learning" can be concerned with translating between discrete update rules which have implicit behaviors, and smooth flows which render such behaviors explicit.
        Then research in  deep learning theory doesn't have to worry about discrete update rules, and can instead focus on understanding curvature-penalized gradient flows.
    </p>
    <p class="body-text">
        A word on mathematical rigor: all else being equal, more rigor is better.
        Indeed, we hope future work can figure out how to make our informal analysis more rigorous.
        Yet, the problem with rigor can function as a handicap. 
        Sometimes, we figure out that a thing is true long before we develop the right mathematical tooling to 
        Thus, while our work is not rigorous, it is ithe first true mechanism of convrgenmce . deep mathematical truehty. which undlir. 
    </p>

    stochastic setting
</body>
</html> 