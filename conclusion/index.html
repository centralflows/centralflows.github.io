<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Conclusion: why central flows?</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <link rel="stylesheet" href="../styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="../js/mathjax-config.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../js/video-controls.js"></script>
</head>
<body>
    <nav class="top-nav">
        <div class="top-nav-left">
            <a href="https://arxiv.org/abs/2410.24206">ðŸ“„ Paper</a>
        </div>
        <div class="top-nav-center">
            <a href="../">Introduction</a>
            <span class="separator">|</span>
            <a href="../part1">Part I</a>
            <span class="separator">|</span>
            <a href="../part2">Part II</a>
            <span class="separator">|</span>
            <a href="../part3">Part III</a>
            <span class="separator">|</span>
            <a href="../conclusion" class="active">Conclusion</a>
        </div>
        <div class="top-nav-right">
            <a href="https://github.com/locuslab/central_flows">ðŸ’» Code</a>
        </div>
    </nav>

    <p class="companion-text">
        This is the companion website for the paper <a href="https://arxiv.org/abs/2410.24206">Understanding Optimization in Deep Learning with Central Flows</a>, published at ICLR 2025.
     </p>

    <h1>Conclusion: why central flows?</h1>

    <p class="body-text">
        Optimization is a core component of deep learning.
        <!-- Regardless of whether you're pre-training a large Transformer to predict the next token, "fine-tuning" this Transformer to become an assistant, or training a diffusion model, you are running an iterative optimization algorithm, -->
        If you are doing deep learning, you are running an iterative optimization algorithm,
        and there are certain optimization-related concerns that always come up:
        What does the learning rate do, and how should I set it?  Why is my gradient behaving like that? Why does one optimizer work better than another on a given task?  How can we build a better optimizer?
        A <strong>theory of optimization for deep learning</strong> would provide a <strong>language for reasoning</strong> about these questions.
    </p>
    <p class="body-text">
        Yet, there is currently no consensus within the community on what form this theory should take.
    In classical optimization theory, there are perhaps two prevalent styles of analysis, yet neither seems well-suited for deep learning.
    </p>
    <p class="body-text">
        First, there are local convergence analyses, which describe the optimizer's behavior in the neighborhood of a minimum.
        The problem with these is that deep learning mostly does not happen in the neighborhood of a minimum.
        In many cases, training terminates before anything remotely resembling a minimum is reached.
        Even if there are situations where training does eventually reach a minimum, the practitioner cares equally about the whole training process, and does not care disproportionately about the last moments.
    </p>
    <p class="body-text">
        Second, there are global convergence analyses, which aim to prove convergence to a minimum from any initialization.
        The problem with these is that, for real networks on real problems, nobody knows how to meaningfully characterize the rate of convergence of any optimizer.
        If we can't meaningfully characterize the rate for any single optimizer, then we can't meaningfully compare two optimizers by comparing their respective rates.
        <!-- It's possible that global convergence analyses only make sense in the context of convex optimization. -->
    </p>
    <p class="body-text">
        Another idea is to analyze optimization in "toy" settings, where the model and the dataset are simple enough for analysis to be tractable.
        But while meaningful results can often be proven about the toy model, it is never clear if or how such results would generalize to practical settings.
        For a practitioner training a real neural net on a real dataset, an analysis of a toy setting does not suggest anything concrete about the problem at hand.
    </p>
    <p class="body-text">
        In light of these limitations, we advocate for a theory of optimization that focuses on studying the <em>local dynamics</em> of the optimizer <em>throughout training</em>.
        (Note that we are <a href="https://arxiv.org/abs/2009.11162">by no means</a> the <a href="https://arxiv.org/abs/2205.10287">first</a> to advocate for this idea.)
        In the deterministic setting, what makes these dynamics particularly interesting are the <em>oscillations</em> that arise from the interaction between the discrete update rule and the local loss landscape.
        These oscillations give rise to rich and often surprising behavior, which we argue is central to understanding how optimization algorithms function.    </p>
    <p class="body-text">
        Our means of analyzing the oscillatory dynamics of an optimization algorithm is to derive a <em>central flow</em> that directly models the optimizer's time-averaged, or smoothed, trajectory.
        This central flow renders <em>explicit</em> the behaviors that were previously <em>implicit</em> in the optimizer's oscillatory dynamics.
        It therefore provides a <b>strictly more informative representation</b> of the optimization algorithm than the "raw" update rule does.
    </p>
    <p class="body-text">
        To be sure, a central flow analysis does not reveal everything we'd like to know about an optimization process.
        For example, our analysis does not allow us to characterize the global rate at which the training loss will decrease.
        For another example, although we say that optimizers implicitly follow a curvature-penalized gradient flow, we do not explain what such a curvature penalty means for learning.
    </p>
    <p class="body-text">
        We regard these (and other) important questions as open problems for deep learning theory that are beyond the scope of the current work.
        In fact, perhaps such questions should be regarded as outside the rightful scope of a "theory of optimization for deep learning".
        Such a theory could focus on translating between discrete update rules which have rich implicit behaviors, and smooth flows which render such behaviors explicit.
        Then the rest of deep learning theory could focus on understanding such smooth flows, without having to think about discrete update rules.
    </p>
    <!-- <p class="body-text">
        A word on mathematical rigor: we agree that 
    </p>

    <p class="body-text">
        A word on mathematical rigor: all else being equal, more rigor is better.
        Indeed, we hope future work can figure out how to make our informal analysis more rigorous.
        Yet, the problem with rigor can function as a handicap. 
        Sometimes, we figure out that a thing is true long before we develop the right mathematical tooling to 
        Thus, while our work is not rigorous, it is ithe first true mechanism of convrgenmce . deep mathematical truehty. which undlir. 
    </p> -->

    <!-- stochastic setting -->
</body>
</html> 